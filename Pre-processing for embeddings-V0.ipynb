{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goals\n",
    "\n",
    "- Reproducible \n",
    "- Quantifiable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "1. #### Don't use standard preprocessing steps like stemming or stopword removal \n",
    "The reason is simple: You loose valuable information, and thereby low performance in identifying signal\n",
    "2. #### percentage coverage of vocabulary and overall coverage percentage\n",
    "3. #### And the text cleaning is embedding specific\n",
    "I just want to mention that the cleaning should be specific for each embedding. For instance, punctuation is present in the Glove embedding, so I believe it should not be removed.\n",
    "4.   #### Glove  Vs  fasttext  <br>\n",
    "5. How are numbers treated in embeddings?  89.999 to ##.### and 29.4 to ##.#\n",
    "6. How is heart symbol treated ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation\n",
    "\n",
    "1. Setting up the preprocessing is eda \n",
    "\n",
    "If a word vector for a token (see remark below for what I mean with token) is available strongly depends on the preprocessing used by the people who trained the embeddings. Unfortunatly most are quite intransparent about this point. (e.g. did they use lower casing, removing contractions, replacement of words, etc. So you need to research their github repositories and/or read the related papers. Google pretrained word vectors replace numbers with \"##\" or the guys training glove twitter embeddings did `text = re.sub(\"<3\", '<HEART>', text)` \n",
    "That all leads to the second conclusion:\n",
    "\n",
    "\n",
    "Similary King- Man + Woman  = King/Queen ? \n",
    "\n",
    "2. Each pretrained embedding needs its own preprocessing\n",
    "\n",
    "If people used different preprocessing for training their embeddings you would also need to do the same, \n",
    "\n",
    "Especially point to can be quite challenging, if you want to concatenate embeddings as in this kernel. Imagine Embedding A preprocesses `\"don't\"` to a single token`[\"dont\"]` and Embedding B to two tokens`[\"do\",\"n't\"]`. You are basically not able to do both. So you need to find a compromise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get your vocabulary as close to the embeddings as possible\n",
    "I will focus in this notebook, how to achieve that. For an example I take the GoogleNews pretrained embeddings, there is no deeper reason for this choice.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 3 main contributions of this notebook are the following:\n",
    "\n",
    "- loading embedding from pickles \n",
    "- aimed preprocessing for GloVe and fasttext vectors (the main content of this notebook)\n",
    "- fixing some unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Put these at the top of every notebook, to get automatic reloading and inline plotting\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import operator\n",
    "import string\n",
    "import gc\n",
    "import random\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import text, sequence\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/swaroop/Downloads/jigsaw-unintended-bias-in-toxicity-classification/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "os.environ['PYTHONHASHSEED'] = str(123)\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use pkl files if possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CRAWL_EMBEDDING_PATH = 'crawl-300d-2M.pkl'\n",
    "GLOVE_EMBEDDING_PATH = 'glove.840B.300d.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have to adjust the load_embeddings function, to handle the pickled dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "\n",
    "def load_embeddings(path):\n",
    "    with open(path,'rb') as f:\n",
    "        emb_arr = pickle.load(f)\n",
    "    return emb_arr\n",
    "\n",
    "def build_matrix(word_index, path):\n",
    "    embedding_index = load_embeddings(path)\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "    unknown_words = []\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_matrix[i] = embedding_index[word]\n",
    "        except KeyError:\n",
    "            unknown_words.append(word)\n",
    "    return embedding_matrix, unknown_words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bad_preprocess(data):\n",
    "    '''\n",
    "    Most common pre-processing used\n",
    "    '''\n",
    "    punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n",
    "    def clean_special_chars(text, punct):\n",
    "        for p in punct:\n",
    "            text = text.replace(p, ' ')\n",
    "        return text\n",
    "\n",
    "    data = data.astype(str).apply(lambda x: clean_special_chars(x, punct))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In principle this functions just deletes some special characters. <br>\n",
    "What is additionally inefficient is that later the keras tokenizer with its default parameters is used which has its own with the above function redundant behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'comment_text'], dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('test.csv')\n",
    "\n",
    "train.columns\n",
    "\n",
    "# test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two important functions which we use throughout this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_coverage(vocab,embeddings_index):\n",
    "    a = {}\n",
    "    oov = {}\n",
    "    k = 0\n",
    "    i = 0\n",
    "    for word in vocab:\n",
    "        try:\n",
    "            a[word] = embeddings_index[word]\n",
    "            k += vocab[word]\n",
    "        except:\n",
    "\n",
    "            oov[word] = vocab[word]\n",
    "            i += vocab[word]\n",
    "            pass\n",
    "\n",
    "    print('Found embeddings for {:.2%} of vocab'.format(len(a) / len(vocab)))\n",
    "    print('Found embeddings for  {:.2%} of all text'.format(k / (k + i)))\n",
    "    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "    return sorted_x\n",
    "\n",
    "def build_vocab(sentences, verbose =  True):\n",
    "    \"\"\"\n",
    "    :param sentences: list of list of words\n",
    "    :return: dictionary of words and their count\n",
    "    \"\"\"\n",
    "    vocab = {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Pkl files or Text files for embedding ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 400000 word vectors in 23.357550859451294s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_embeddings_test(path):\n",
    "    with open(path) as f:\n",
    "        return dict(get_coefs(*line.strip().split(' ')) for line in f)\n",
    "\n",
    "tic = time.time()\n",
    "embedding_index = load_embeddings_test('/home/swaroop/Downloads/glove.6B/glove.6B.300d.txt')\n",
    "print(f'loaded {len(embedding_index)} word vectors in {time.time()-tic}s')\n",
    "\n",
    "del embedding_index\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 2196008 word vectors in 6.887946128845215s\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "glove_embeddings = load_embeddings('glove.840B.300d.pkl')\n",
    "print(f'loaded {len(glove_embeddings)} word vectors in {time.time()-tic}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Loading pkl files is twenty times faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab(list(train['comment_text'].apply(lambda x:x.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 36.63% of vocab\n",
      "Found embeddings for  91.44% of all text\n"
     ]
    }
   ],
   "source": [
    "crawl_embeddings = load_embeddings('crawl-300d-2M.pkl')\n",
    "oov = check_coverage(vocab,crawl_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"Trump's\", 1220),\n",
       " (\"aren't\", 1168),\n",
       " (\"Don't\", 1080),\n",
       " (\"wouldn't\", 1067),\n",
       " ('Yes,', 997),\n",
       " (\"wasn't\", 931),\n",
       " (\"Let's\", 763),\n",
       " (\"You're\", 752),\n",
       " ('So,', 709),\n",
       " (\"He's\", 672)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 32.62% of vocab\n",
      "Found embeddings for  89.70% of all text\n"
     ]
    }
   ],
   "source": [
    "oov = check_coverage(vocab,glove_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"isn't\", 2228),\n",
       " (\"That's\", 1974),\n",
       " (\"won't\", 1678),\n",
       " (\"he's\", 1319),\n",
       " (\"Trump's\", 1220),\n",
       " (\"aren't\", 1168),\n",
       " (\"wouldn't\", 1067),\n",
       " ('Yes,', 997),\n",
       " (\"they're\", 966),\n",
       " (\"wasn't\", 931)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2196008\n",
      "2000000\n"
     ]
    }
   ],
   "source": [
    "print (len(glove_embeddings))\n",
    "print (len(crawl_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like `'` and other punctuation directly on or in a word is an issue. <br>\n",
    "We could simply delete punctuation to fix that words, but there are better methods.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "latin_similar = \"’'‘ÆÐƎƏƐƔĲŊŒẞÞǷȜæðǝəɛɣĳŋœĸſßþƿȝĄƁÇĐƊĘĦĮƘŁØƠŞȘŢȚŦŲƯY̨Ƴąɓçđɗęħįƙłøơşșţțŧųưy̨ƴÁÀÂÄǍĂĀÃÅǺĄÆǼǢƁĆĊĈČÇĎḌĐƊÐÉÈĖÊËĚĔĒĘẸƎƏƐĠĜǦĞĢƔáàâäǎăāãåǻąæǽǣɓćċĉčçďḍđɗðéèėêëěĕēęẹǝəɛġĝǧğģɣĤḤĦIÍÌİÎÏǏĬĪĨĮỊĲĴĶƘĹĻŁĽĿʼNŃN̈ŇÑŅŊÓÒÔÖǑŎŌÕŐỌØǾƠŒĥḥħıíìiîïǐĭīĩįịĳĵķƙĸĺļłľŀŉńn̈ňñņŋóòôöǒŏōõőọøǿơœŔŘŖŚŜŠŞȘṢẞŤŢṬŦÞÚÙÛÜǓŬŪŨŰŮŲỤƯẂẀŴẄǷÝỲŶŸȲỸƳŹŻŽẒŕřŗſśŝšşșṣßťţṭŧþúùûüǔŭūũűůųụưẃẁŵẅƿýỳŷÿȳỹƴźżžẓ\"\n",
    "white_list = string.ascii_letters + string.digits + latin_similar + ' '\n",
    "white_list = string.ascii_letters + string.digits  + ' '\n",
    "white_list += \"'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "',.\":)(-!?|;$&/[]>%=#*+\\\\•~@£·_{}©^®`<→°€™›♥←×§″′Â█½à…“★”–●â►−¢²¬░¡¶↑±¿▾═¦║―¥▓—‹─▒：¼⊕▼▪†■’▀¨▄♫☆é¯♦¤▲è¸¾Ã⋅‘∞∙）↓、│（»，♪╩╚³・╦╣╔╗▬❤ïØ¹≤‡√◄━⇒▶º≥╝♡◊。✈≡☺✔↵≈ã✓Ð♣☎℃◦ø└‟Å～！○◆№♠▌✿▸⁄□É❖í✦．÷｜À┃å／￥╠↩✭▐☼µ☻┐Ó├ü«á∼┌℉☮฿≦♬✧〉－⌂✖･◕※‖◀‰\\x97↺æ∆Ñœ┘┬╬،⌘š⊂Îª＞〈⎙Å？☠⇐▫∗∈≠♀ñƒ♔˚ç℗┗＊┼❀äı＆∩♂‿∑‣➜┛⇓☯⊖☀┳；∇⇑✰◇♯☞´ə↔┏｡ß◘∂Û✌♭ó┣┴┓✨ÖÄˈ˜❥┫Ü℠✒ž［∫\\x93≧］\\x94∀♛\\x96∨◎ˑö↻⅓Æ⇩＜≫✩ˆ✪È♕Ù؟₤☛Ç╮␊＋┈ɡ％╋▽⇨┻þ⊗Á￡।▂✯▇＿➤ô₂✞＝▷△Þ◙î▅✝ﾟÏ∧␉☭ð┊╯☾➔ê∴\\x92▃↳＾׳ú➢╭➡＠⊙ì☢˝Ô⅛∏ā„①๑∥❝Š☐▆Ÿûý╱⋙๏☁⇔▔\\x91②➚◡Ê╰Ì٠ë♢Ý˙۞✘✮☑⋆ℓⓘ❒☣✉ē⌊➠∣❑⅔◢Òòⓒ\\x80〒Í∕▮⦿✫✚⋯♩☂ˌ❞‗č܂☜ī‾✜╲ù∘⟩ō＼⟨·⅜✗Ă♚∅Ëⓔ◣͡‛❦⑨③◠✄❄１∃␣≪｢≅◯☽２İ∎｣⁰❧̅ÿǡⒶ↘⚓▣˘∪⇢Ú✍ɛ⊥＃⅝⎯õ↠۩☰Õ◥⊆✽ﬁ⚡↪ở❁☹ł◼☃◤❏Žⓢ⊱α➝̣✡∠｀▴┤Ȃ∝♏ⓐ✎;３④␤＇❣⅞✂✤ⓞ☪✴⌒˛♒＄ɪ✶▻Ⓔ◌◈۲Ʈ❚ʿ❂￦◉╜̃ťν✱╖❉₃ⓡℝ٤↗❶ʡ۰ˇⓣ♻➽۶₁ʃ׀✲Đʤ✬☉▉≒☥⌐♨✕ⓝ⊰❘＂⇧̵➪４▁βđ۱▏⊃ⓛ‚♰́✏⏑Œ̶٩Ⓢー⩾日￠❍≃⋰♋ɿ､̂ǿ❋✳ⓤ╤▕⌣✸℮⁺▨⑤╨Ⓥ♈❃☝Ā５✻⊇≻♘♞◂７✟Łū⌠✠☚✥ŋ❊ƂⒸŮ⌈❅Ⓡ♧Ⓞɑλ۵▭❱Ⓣ∟☕♺∵⍝ⓑɔ✵ŕ✣ℤ年ℕ٭♆Ⓘⅆ∶⚜◞்✹Ǥȡ➥ᴥ↕ɂ̳∷✋į➧∋̿ͧʘ┅⥤⬆ǀμ₄⋱ʔ☄↖⋮۔♌Ⓛ╕♓ـ⁴❯♍▋ă✺⭐６✾♊➣▿Ⓑ♉Ａ⏠◾▹⑥⩽в↦ż╥⍵⌋։➨и∮⇥ⓗⒹ⁻ʊć⎝⌥⌉◔◑ǂ✼♎ℂ♐╪ɨ⊚☒⇤θВⓜ⎠Ｏ◐ǰ⚠╞ﬂş◗⎕ⓨ☟Ｉⓟ♟❈↬ⓓŞ◻♮❙а♤∉؛⁂例ČⓃ־♑╫╓╳⬅☔πɒɹ߂Ō☸ɐʻ┄╧ʌ׃８ʒ⎢ġ❆⋄⚫ħ̏☏➞͂␙Ⓤ◟Ƥąʕ̊Ȥ⚐✙は↙̾ωΔ℘ﾞ✷⑦φ⍺❌⊢▵✅ｗ９ⓖ☨▰ʹŢ╡Ⓜő☤∽╘Ű˹↨ȿ♙⬇♱ś⌡Ω⠀╛❕┉Ⓟ̀Ǩ♖ⓚ┆⑧⎜Śǹ◜⚾⤴✇╟⎛☩➲➟ⓥⒽŘ⏝Ŀ◃０₀╢月↯✆ĶĢ˃⍴Ĥ❇ũ⚽╒Ｃɻɤ̸ʼ♜☓Ｔ➳⇄γ☬⚑✐⁵δȭ⌃◅▢ｓȸ❐ě∊☈ⅇℜ॥σ⎮ȣ▩のτεřＳŀு⊹‵␔☊➸̌☿⇉Ĺ➊⊳╙⁶ⓦ⇣｛̄↝ź⎟ęℳŹ▍❗ℑＭɾſｍŧĦ״Γ΄▞◁⛄⇝Ż⎪ˤ♁ｖ⇠☇ǻ✊位ℒạி｝๐⭕ĺ➘Ｂ❺ɸˡ⁀⑩ｃ⅕ŪƼ۳☙❛ɣ₆ŷƪ❓⟲Ʒ⇀≲Ｐ❷١ńŦŐⓕ⎥ＤсŔ\\u06ddǥͤ₋̱Ń̎♝Ľ≳▙ＲŤḥʹ➭ℰ܀ğʺȫⒼ⇛ˉ▊❸Ǻ号⇗̷Ŵ⇱℅Ⓧ⚛̐ːǴʱ̕⇌ŏＦｏ␀≌๖Ⓦ⊤̓ď☦ĭĳℚⒻɚ▜ʋʞ➙Ⓨ⌨◮☷ǁ◍ĮⓀ≔ų⏩⍳℞┋ɲ˻▚₅≺ْĒƱ▟➻Ďţ̪ŭ⏪̉❼⎞ρȥʂ┇⍟⇪ʧ▎⇦␝ＥỞľ⤷➀≖ＵＱ⟶♗ℵ̴♄⅙ͨ̈Ǭ❜̡ṣǫ▛✁➩ĸா˂↥Ī⏎⎷Ć̲➖ģĄʐɽ↲ℬℏ⩵̗Ｊ由η❢₈Ɣ≎➋ΣＬƖ❹Ĵǵ⚔⇇̑⊿ŬĞĚĥ۴⁸ᶘ̖ǾơΦ☍ȯ➹ĉ⥊⁁✢ａɯ回ĵ'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_chars = ''.join([c for c in glove_embeddings if len(c) == 1])\n",
    "glove_symbols = ''.join([c for c in glove_chars if not c in white_list])\n",
    "glove_symbols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see all symbols that we have an embedding vector for. Intrestingly its not only a vast amount of punctuation but also emojis and other symbols. <br><br>\n",
    "\n",
    "Especially when doing sentiment analysis emojis and other symbols carrying sentiments should not be deleted! <br><br>\n",
    "\n",
    "What we can delete are symbols we have no embeddings for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.,?\\n!&-\":()%/>…;ï`“#”[]<+$’*—🐴=ĺ_جماعةلفقرءé‘–ìˈʊɒ☐🐢~😂{}@😉\\xad☑₂\\u200b•😐😊―◄►ē^«à»ᴛᴀʀᴡᴏᴋɪɴɢғᴍʜᴇᴊʙᴜᴅʏᴄʟᴘᴠō☠️\\xa0\\tᴵ\\\\😃😦😇😪πολυμαθής😀☮\\u200a☒§😳™𝐧𝐚𝐭𝐢𝐨ñ🙏🙂¢𝒑𝒓𝒆𝒔𝒊𝒅𝒏𝒕𝒂𝒖𝒈𝒉🙄😁😑💩💨çêá😥‑ˌɔːʒɑə☄\\u2009●°🤔😩😟😭👿č£ü·ā\\x7fíóäÉ☺🎉º\\u202a\\u202c´😰€\\u200e☹✔🤡⩛⏖☁℅½😎èÁ👍🇺🇸¼▰τῦ᾽ἔδεισνἡβκὶὺἐῖάηῶρέωξγόὲΜἈίὰὑῆζῷῳἰχύφἀώὸὴὡἄὀὐᾶὅἑἅὖʃʌ🎭．😈ʻ✌🇾😔𝒎\\u2028ú◞◝|😜➤©》《😆☀😘͞ūâΔ𝙥𝙤𝙖𝙯𝙞𝙩𝙀𝙒𝙬😢œ\\u2004▀▔😮Ā😝👏𝒃𝒄😬👌ɡ👎😄💔\\x85\\x10𝐡𝐩𝐬𝐰𝐲𝐮𝐛𝐞𝐜𝐦𝐯𝐳𝐔𝐱𝟔𝟓𝐅𝐓𝐃𝐘šć😛💜²ᵗʰ❤≠𝒐𝒗𝒙𝑰𝒇𝒚𝒌𝒘𝒍𝑴😏💝\\ufeffö\\u200fдерьмо🕍𝑹😣↩\\x08¡Двяйнп😍卐卍🕊⅔🐵🏻ò小土豆አዎа\\u2005⒈⒉⒊⒋⒌⒍🌈▱⅓ğ℠💥⚾⚽☻Ô∙�ë⁎⁍̴̛ᴗḵ̱🙈🌎🙌🏽🌞🍀😅🚂♥īã♡♫♪ἕ😧🙁Öî🏼😱🤚💰😡\\x81😲🎾🤣ὠℐ\\uf070\\uf071\\uf03d\\uf031\\uf02f\\uf032\\uf028\\uf02d\\uf061\\uf029\\uf020¬∆РсиАлкчтбышужгô\\uf0b7😒ŋфз💤😤💀═★☆🤷\\u200d♂🔥༼つ◕༽𝙨𝙢𝙙𝙚𝙧𝙣𝙜𝙛𝙡𝙪𝙘🙃👀▆→○\\r😞øÙ🌙𝒀💫'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jigsaw_chars = build_vocab(list(train[\"comment_text\"]))\n",
    "jigsaw_symbols = ''.join([c for c in jigsaw_chars if not c in white_list])\n",
    "jigsaw_symbols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically we can delete all symbols we have no embeddings for:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n🐴جماعةلفقرء🐢😂😉\\xad\\u200b😐😊ᴛᴀʀᴡᴏᴋɴɢғᴍʜᴇᴊʙᴜᴅʏᴄʟᴘᴠ️\\xa0\\tᴵ😃😦😇😪ουής😀\\u200a😳𝐧𝐚𝐭𝐢𝐨🙏🙂𝒑𝒓𝒆𝒔𝒊𝒅𝒏𝒕𝒂𝒖𝒈𝒉🙄😁😑💩💨😥‑\\u2009🤔😩😟😭👿\\x7f🎉\\u202a\\u202c😰\\u200e🤡⩛⏖😎👍🇺🇸ῦ᾽ἔιἡκὶὺἐῖάῶέξόὲΜἈίὰὑῆζῷῳἰχύἀώὸὴὡἄὀὐᾶὅἑἅὖ🎭😈🇾😔𝒎\\u2028◝😜》《😆😘͞𝙥𝙤𝙖𝙯𝙞𝙩𝙀𝙒𝙬😢\\u2004😮😝👏𝒃𝒄😬👌👎😄💔\\x85\\x10𝐡𝐩𝐬𝐰𝐲𝐮𝐛𝐞𝐜𝐦𝐯𝐳𝐔𝐱𝟔𝟓𝐅𝐓𝐃𝐘😛💜ᵗʰ𝒐𝒗𝒙𝑰𝒇𝒚𝒌𝒘𝒍𝑴😏💝\\ufeff\\u200fдерьмо🕍𝑹😣\\x08Дяйнп😍卐卍🕊🐵🏻小土豆አዎ\\u2005⒈⒉⒊⒋⒌⒍🌈▱💥�⁎⁍̛ᴗḵ🙈🌎🙌🏽🌞🍀😅🚂ἕ😧🙁🏼😱🤚💰😡\\x81😲🎾🤣ὠℐ\\uf070\\uf071\\uf03d\\uf031\\uf02f\\uf032\\uf028\\uf02d\\uf061\\uf029\\uf020РАлкчтбышужг\\uf0b7😒фз💤😤💀🤷\\u200d🔥༼つ༽𝙨𝙢𝙙𝙚𝙧𝙣𝙜𝙛𝙡𝙪𝙘🙃👀\\r😞🌙𝒀💫'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbols_to_delete = ''.join([c for c in jigsaw_symbols if not c in glove_symbols])\n",
    "symbols_to_delete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The symbols we want to keep we need to isolate from our words. So lets setup a list of those to isolate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.,?!&-\":()%/>…;ï`“#”[]<+$’*—=ĺ_é‘–ìˈʊɒ☐~{}@☑₂•―◄►ē^«à»ɪō☠\\\\πλμαθ☮☒§™ñ¢çêáˌɔːʒɑə☄●°č£ü·āíóäÉ☺º´€☹✔☁℅½èÁ¼▰τδεσνβηρωγφʃʌ．ʻ✌ú◞|➤©☀ūâΔœ▀▔Āɡšć²❤≠ö↩¡в⅔òа⅓ğ℠⚾⚽☻Ô∙ë̴̱♥īã♡♫♪Öî¬∆сиôŋ═★☆♂◕▆→○øÙ'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbols_to_isolate = ''.join([c for c in jigsaw_symbols if c in glove_symbols])\n",
    "symbols_to_isolate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next comes the next trick. Instead of using an inefficient loop of `replace` we use `translate`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "isolate_dict = {ord(c):f' {c} ' for c in symbols_to_isolate}\n",
    "remove_dict = {ord(c):f'' for c in symbols_to_delete}\n",
    "\n",
    "\n",
    "def handle_punctuation(x):\n",
    "    x = x.translate(remove_dict)\n",
    "    x = x.translate(isolate_dict)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You look like a horse ?   '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_punctuation('You look like a horse?  🐴' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So lets apply that function to our text and reasses the coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['comment_text'] = train['comment_text'].apply(lambda x:handle_punctuation(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 76.75% of vocab\n",
      "Found embeddings for  98.75% of all text\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(\"isn't\", 2329),\n",
       " (\"That's\", 1997),\n",
       " (\"won't\", 1763),\n",
       " (\"he's\", 1359),\n",
       " (\"Trump's\", 1243),\n",
       " (\"aren't\", 1223),\n",
       " (\"wouldn't\", 1085),\n",
       " (\"they're\", 989),\n",
       " (\"wasn't\", 963),\n",
       " (\"there's\", 836)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = build_vocab(list(train['comment_text'].apply(lambda x:x.split())))\n",
    "oov = check_coverage(vocab,glove_embeddings)\n",
    "oov[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_contractions(x):\n",
    "    x = tokenizer.tokenize(x)\n",
    "    x = ' '.join(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['comment_text'] = train['comment_text'].apply(lambda x:handle_contractions(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 81.80% of vocab\n",
      "Found embeddings for  99.57% of all text\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Brexit', 136),\n",
       " ('tRump', 134),\n",
       " (\"gov't\", 97),\n",
       " ('theglobeandmail', 69),\n",
       " (\"'the\", 64),\n",
       " ('theguardian', 59),\n",
       " ('deplorables', 56),\n",
       " ('Drumpf', 52),\n",
       " (\"'The\", 42),\n",
       " (\"'bout\", 40)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = build_vocab(list(train['comment_text'].apply(lambda x:x.split())),verbose=False)\n",
    "oov = check_coverage(vocab,glove_embeddings)\n",
    "oov[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the oov words look \"normal\", apart from those still carrying the `'` token in the beginning of the word. Will need to fix those \"per hand\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_quote(x):\n",
    "    x = [x_[1:] if x_.startswith(\"'\") else x_ for x_ in x]\n",
    "    x = ' '.join(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['comment_text'] = train['comment_text'].apply(lambda x:fix_quote(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Jeff Sessions is another one of Trump s Orwell...\n",
       "1    I actually inspected the infrastructure on Gra...\n",
       "2    No it wo n't . That s just wishful thinking on...\n",
       "3    Instead of wringing our hands and nibbling the...\n",
       "4    how many of you commenters have garbage piled ...\n",
       "Name: comment_text, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['comment_text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 84.13% of vocab\n",
      "Found embeddings for  99.66% of all text\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Brexit', 139),\n",
       " ('tRump', 134),\n",
       " (\"gov't\", 97),\n",
       " ('theglobeandmail', 69),\n",
       " ('theguardian', 59),\n",
       " ('deplorables', 58),\n",
       " ('Drumpf', 52),\n",
       " (\"Gov't\", 39),\n",
       " ('Trumpcare', 38),\n",
       " ('garycrum', 37),\n",
       " ('Klastri', 29),\n",
       " ('SB91', 27),\n",
       " ('Trumpism', 24),\n",
       " (\"y'all\", 23),\n",
       " ('Saullie', 21),\n",
       " ('financialpost', 21),\n",
       " ('HIHS', 20),\n",
       " ('bigly', 20),\n",
       " ('907AK', 19),\n",
       " ('klastri', 19),\n",
       " ('Trumpian', 19),\n",
       " ('Hoopili', 19),\n",
       " ('Trumpsters', 18),\n",
       " ('Auwe', 18),\n",
       " ('l2g', 18),\n",
       " ('civilbeat', 17),\n",
       " ('shibai', 17),\n",
       " ('Trudeaus', 17),\n",
       " ('SHOPO', 16),\n",
       " ('Donkel', 16),\n",
       " ('cashapp24', 16),\n",
       " ('Deplorables', 15),\n",
       " ('Koncerned', 15),\n",
       " ('Meggsy', 15),\n",
       " ('BCLibs', 14),\n",
       " ('Zupta', 14),\n",
       " ('Anbang', 14),\n",
       " ('Layla4', 14),\n",
       " ('gofundme', 14),\n",
       " ('Crapwell', 13),\n",
       " ('wiliki', 13),\n",
       " ('xbt', 13),\n",
       " ('TFWs', 13),\n",
       " ('vancouversun', 13),\n",
       " ('tRUMP', 13),\n",
       " ('SJWs', 12),\n",
       " ('TrumpCare', 12),\n",
       " ('FakeNews', 12),\n",
       " ('MAGAphants', 12),\n",
       " ('MUPTE', 12)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = build_vocab(list(train['comment_text'].apply(lambda x:x.split())),verbose=False)\n",
    "oov = check_coverage(vocab,glove_embeddings)\n",
    "oov[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good. Now we can implement the preprocessing functions and train a model. See "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.26362    0.17013   -0.2353     0.23      -0.17085    0.31102\n",
      "  0.45467    0.24224    0.099638   1.0841     0.3914    -0.58338\n",
      "  0.23517    0.19334   -0.12729   -0.25476   -0.54572   -0.32191\n",
      "  0.18551    0.53016    0.010809   0.64877    0.38942    0.027237\n",
      " -0.21348    0.44491   -0.3409     0.1347    -0.25356   -0.093586\n",
      "  0.60238   -0.48313    0.55752    0.20925   -0.091584  -0.1464\n",
      " -0.22533   -0.42055   -0.098816  -0.42965   -0.53209   -0.24078\n",
      "  0.2644     0.15721    0.16086    0.28977   -0.57275   -0.23805\n",
      "  0.12122   -0.53071   -0.25444   -0.31427   -0.24907    0.28778\n",
      "  0.62412   -0.15069   -0.66065   -0.37106    0.22876   -0.43941\n",
      "  0.40445   -0.12752    0.49618    0.13293    0.5845     0.22621\n",
      "  0.37511    0.090205   0.06014    0.070341   0.53625   -0.077997\n",
      "  0.41538    0.41096   -0.79239    0.61845    0.43017    0.14255\n",
      " -0.11131   -0.14772   -0.009152  -0.12959    0.13898    0.44132\n",
      " -0.42023    0.1564    -0.021194  -0.084832  -0.21505   -0.0074104\n",
      " -0.020073   0.030071   0.22421   -0.32244    0.061356  -0.32725\n",
      "  0.18955   -0.014523  -0.22573    0.32862    0.18631    0.32265\n",
      " -0.41113   -0.11265   -0.68734    0.54795    0.41112    0.38602\n",
      "  0.42553   -0.48602    0.29566   -0.50629   -0.016576   0.042985\n",
      " -0.43122    0.02232   -0.15343   -0.81653    0.44244    0.12006\n",
      " -0.11674   -0.18427   -0.20327   -0.11445   -0.32223    0.072973\n",
      "  0.088273   0.23628   -0.34588   -0.11804   -0.37184    0.11431\n",
      "  0.46233   -0.061556  -0.50881    0.56843   -0.22537    0.038522\n",
      " -0.19632    0.04598    1.1233    -0.45106   -0.46668   -0.59176\n",
      " -0.19079    0.12035    0.19483   -0.0066871 -0.40029   -0.044806\n",
      " -0.093634   0.013256  -0.40135    0.0025535  0.079055  -0.17925\n",
      "  0.14034    0.29264   -0.40531   -0.028683  -0.12945    0.098471\n",
      "  0.15787    0.33536   -0.33332    0.10416    0.019319   0.22233\n",
      " -0.058894  -0.0018831  0.057077  -0.54798    0.10414   -0.32282\n",
      " -0.00648   -0.29746    0.41156    0.1531     0.036065   0.12672\n",
      "  0.12578   -0.27992   -0.13379   -0.10844   -0.28658    0.17549\n",
      "  0.17214    0.050928  -0.33919    0.018806  -0.23711   -0.010031\n",
      "  0.12607    0.16859    0.17231    0.59395    0.044558   0.25455\n",
      "  0.3747     0.10992    0.12295   -0.54739    0.073754  -0.022377\n",
      " -0.0988    -0.22509    0.42308   -0.24483   -0.23081    0.1437\n",
      " -0.059996   0.44895    0.080195   0.008131   0.44337    0.085603\n",
      "  0.11893   -0.29375    0.50861   -0.15758   -0.006057   0.57884\n",
      " -0.06318    0.26126   -0.066781  -0.35295    0.49255    0.23399\n",
      "  0.021624  -0.13199   -0.0043879 -0.16376    0.14731   -0.056233\n",
      " -0.12239   -0.47253   -0.0044024  0.020082   0.092347  -0.34829\n",
      " -0.13317    0.61168   -0.23679    0.27465    0.38759   -0.22593\n",
      "  0.015215   0.16867    0.37828    0.80502    0.332     -0.21969\n",
      " -0.26031    0.096206  -0.20126   -0.87652   -0.24232    0.4657\n",
      "  0.29853    0.54113    0.091725   0.26665   -0.0013598  0.15512\n",
      "  0.088698  -0.025479   0.020547   0.16778    0.03961   -0.045912\n",
      " -0.062889   0.17909    0.48027   -0.23234   -0.4194    -0.15814\n",
      "  0.0019929 -0.2351     0.46941   -0.62223   -0.35209   -0.24155\n",
      " -0.10067   -0.0298    -0.36743   -0.02381   -0.45915   -0.18043\n",
      " -0.12538    0.23158    0.11693    0.3463     0.22112    0.0063201\n",
      " -0.22869   -0.20357    0.19431    0.13362    0.51823    0.11898  ]\n",
      "[ 9.0559e-02  6.3740e-01  2.2783e-01  7.8112e-01 -1.2332e+00 -1.6770e-01\n",
      "  7.1038e-01  4.2668e-01  1.2305e+00 -8.1171e-01 -4.8620e-01  1.0066e+00\n",
      "  7.4941e-01  1.1884e+00  4.4158e-01  9.4462e-01 -4.4574e-01 -2.5826e-01\n",
      "  5.9727e-01  6.2066e-01 -5.9156e-01  3.5713e-02  5.9170e-01 -5.5889e-01\n",
      " -2.5015e-01 -2.0386e-01  5.1234e-01  1.7046e-01  1.3444e+00  9.0840e-01\n",
      " -5.3409e-01 -8.6611e-01 -4.3777e-01  1.2017e+00 -5.3947e-01  6.3008e-01\n",
      "  2.1274e-01 -1.8045e-02  2.1991e-01 -7.5474e-01 -8.2863e-01 -4.1981e-02\n",
      "  8.0563e-01 -1.9737e-01 -3.9347e-02 -4.6686e-01  4.2254e-01 -6.8618e-01\n",
      "  7.8305e-02  8.3055e-01 -7.5993e-01 -1.3096e+00 -5.1085e-01 -8.7646e-01\n",
      " -9.8880e-01 -5.3825e-01 -1.0658e-01 -2.4156e-01  8.2193e-01 -1.0718e-01\n",
      " -2.2716e-01 -5.6647e-01 -6.9409e-01 -3.0210e-01  5.7434e-01 -5.7051e-02\n",
      " -1.8397e+00  6.5003e-01 -1.6222e-01 -4.2384e-01  5.5367e-01  7.8579e-01\n",
      " -1.1964e+00 -5.0341e-01  3.1684e-01  1.0399e+00  6.8322e-01 -1.7805e-01\n",
      "  4.6458e-01  3.5628e-01 -6.8683e-01  8.7271e-01 -2.9166e-01  6.9346e-01\n",
      " -7.6361e-01  8.8709e-01  4.4508e-01 -3.1203e-01  5.7399e-01 -5.2332e-01\n",
      " -6.2179e-01  5.5176e-01 -4.0906e-01  6.4777e-02  5.1499e-04  3.3233e-01\n",
      " -2.0939e-01 -5.7052e-02 -2.2156e-01  2.0624e+00 -2.6505e-01 -2.5567e-03\n",
      "  1.0322e-01  9.0207e-01 -9.7647e-01 -1.7998e+00  4.4910e-01  7.5580e-01\n",
      "  2.6646e-01 -7.9571e-01 -1.1177e+00  8.7087e-01 -5.2629e-01 -8.5339e-01\n",
      " -1.4652e-01  1.7843e-01 -5.0109e-01 -2.1974e-01  3.6618e-01  3.4809e-01\n",
      " -4.9330e-01  1.3194e+00 -2.3433e-03  6.0274e-01 -4.7061e-01  2.2322e-01\n",
      "  4.3894e-01 -7.0602e-01  9.6187e-01  4.6658e-01  3.1589e-01 -1.3832e-04\n",
      " -1.8836e-01  1.0214e+00 -7.7827e-01  1.3066e-01  6.5394e-01 -3.2873e-03\n",
      " -8.8337e-01  3.2390e-01  7.0473e-01  2.5234e-01 -6.4763e-01 -1.1669e-01\n",
      " -6.2621e-01  8.0425e-02  3.8291e-01  2.4666e-01 -2.6608e-01  4.6203e-02\n",
      " -1.0805e-01  1.3079e-01 -1.3775e+00 -1.0802e+00 -2.7774e-01  6.0275e-01\n",
      " -1.5507e-01  1.6173e+00 -1.0538e+00  8.2191e-01 -3.9127e-01  2.6572e-01\n",
      "  3.1101e-01  1.7988e-02  4.2064e-01 -2.5676e-01 -6.8710e-01  7.9513e-01\n",
      " -1.3524e+00 -1.9394e-02  3.8395e-01 -1.2740e-01  2.9831e-01 -1.0633e-01\n",
      "  4.2832e-01  1.5714e-01  1.0828e+00 -6.4269e-01 -8.1599e-02  1.0859e-01\n",
      " -5.0306e-01  5.5940e-01  1.2550e-01 -1.3653e+00  8.8866e-01  1.5890e+00\n",
      " -7.9465e-01  7.4450e-01 -1.3201e-01  2.4605e-01  6.7047e-02 -6.8251e-01\n",
      "  5.5472e-01 -2.4661e+00  1.0322e+00  8.6812e-01 -5.8707e-01  2.5278e-01\n",
      "  9.4953e-01  8.2357e-02 -1.0406e-01  1.2017e-01  3.9654e-01  1.5038e-01\n",
      " -6.0454e-01 -1.2230e-01 -9.0852e-02  2.9181e-01 -6.1883e-01 -5.7364e-01\n",
      " -4.0741e-01  7.4959e-01 -4.5572e-01  1.1555e+00 -1.6657e-01  4.8746e-01\n",
      "  6.1045e-01 -6.1847e-01  6.9974e-01  1.7733e+00 -4.2715e-01  1.2316e+00\n",
      " -3.7066e-01 -6.4014e-01 -6.3114e-01 -7.2142e-01 -5.8875e-01 -3.3342e-01\n",
      "  7.7574e-01 -5.7476e-01  4.9404e-01 -4.2346e-02  1.2870e-01 -8.0267e-01\n",
      " -7.9322e-01 -6.4663e-01 -6.0973e-01  2.8335e-01 -1.2820e+00  1.0197e+00\n",
      "  3.0448e-01  2.9338e-01 -6.9419e-01  4.8361e-01  3.4193e-01  3.7895e-01\n",
      "  3.1690e-01 -7.6694e-01  7.1825e-01 -3.5286e-01  3.2869e-01 -4.6506e-02\n",
      "  6.9330e-01  1.3865e-01  9.6564e-02  6.1661e-01 -1.7762e-01  7.5039e-01\n",
      " -1.2545e+00  2.5940e-01 -1.4427e+00  4.3372e-01 -1.4571e+00  2.8345e-01\n",
      "  4.4586e-01  1.0788e+00  1.9953e-01 -2.9289e-01 -1.0838e+00  9.5235e-01\n",
      " -4.4520e-01  9.5736e-02  4.0918e-01 -4.6975e-01  4.8382e-01 -3.6931e-01\n",
      " -5.6103e-01 -6.0439e-01 -1.0673e+00  3.1492e-01  7.7096e-01  6.6203e-01\n",
      " -9.6485e-01 -1.5699e+00  8.4950e-01 -2.8417e-01  6.2853e-01 -1.0103e+00\n",
      "  9.4356e-01  6.5254e-01  5.8450e-01 -6.4071e-01 -4.6136e-01  1.1951e+00\n",
      "  1.6465e-01 -8.1190e-01  7.1805e-01  4.6926e-01 -3.8581e-01  1.5690e-01]\n",
      "[ 0.27756   -0.15478    0.53727    0.15117    0.20753   -0.014\n",
      " -0.1732     0.26043    0.038549   0.69817    0.51677   -0.55199\n",
      " -0.0746    -0.026628   0.29144    0.14717    0.16175    0.33972\n",
      " -0.30425    0.29344    0.36554   -0.46548    0.078528  -0.52653\n",
      " -0.020706   0.23888    0.013617   0.33465    0.10661    0.49333\n",
      "  0.41214    0.21697    0.24945    0.29676   -0.22977   -0.31877\n",
      " -0.10705   -0.06481    0.20905   -0.044537  -0.05336    0.26116\n",
      "  0.35861   -0.2299     0.36582   -0.079682   0.19234   -0.058312\n",
      " -0.46523   -0.54574   -0.10576    0.37161   -0.55057    0.5414\n",
      "  0.24026    0.072915   0.075191  -0.35432    0.21722    0.084252\n",
      " -0.062981   0.1217     0.69621   -0.61859    0.31434   -0.41222\n",
      "  0.52764    0.31797   -0.18462   -0.31199   -0.21415   -0.3299\n",
      " -0.45317   -0.053962   0.30811   -0.025958   0.057586   0.19678\n",
      "  0.17249    0.43741   -0.42169    0.33271    0.37142    0.55853\n",
      "  0.20937    0.97311   -0.22219    0.068848  -0.0084583  0.39965\n",
      "  0.39752    0.19538    0.023742  -0.025816  -0.059601   0.40892\n",
      "  0.19726    0.35162   -0.32674    0.58588   -0.1108     0.06179\n",
      "  0.22092    0.0075179 -0.31395   -2.0178     0.17333    0.20302\n",
      "  0.21393   -0.021743  -0.046042  -0.042471   0.16485   -0.60798\n",
      " -0.26653    0.79127    0.45237   -0.51888   -0.21463   -0.52562\n",
      " -0.30886    0.051069   0.29041    0.44473    0.072916  -0.10384\n",
      "  0.31788    0.3491     0.046472  -0.22372    0.275     -0.081097\n",
      " -0.47755   -0.41829   -0.2218    -0.055947  -0.16915   -0.21734\n",
      "  0.16404   -0.50143   -0.67822   -0.37024   -0.040074  -0.066548\n",
      "  0.10342   -0.0051621  0.33593   -0.027162  -0.3012     0.2006\n",
      "  0.55094   -0.4124    -0.18667   -0.10806    0.12187    0.14017\n",
      " -0.035657   0.010235   0.13853    0.57915    0.17921    0.23403\n",
      "  0.092967   0.16774    0.41492   -0.2306     0.11749   -0.18266\n",
      " -0.31518   -0.31301   -0.035314  -0.072206  -0.29925   -0.11514\n",
      "  0.23633   -0.25579    0.51853    0.37701   -0.19391   -0.026225\n",
      " -0.21287    0.28728    0.38016   -0.12086   -0.1276    -0.044182\n",
      " -0.047278  -0.32628   -0.48452   -0.12065   -0.31979    0.11109\n",
      "  0.39068   -0.12073    0.38756   -0.048043  -0.043261   0.21345\n",
      "  0.14608    0.082047   0.36227    0.067099   0.2057    -0.30103\n",
      "  0.032852   0.3546    -0.26375   -0.70046    0.11682   -0.26967\n",
      "  0.20043    0.29647    0.094495   0.36436   -0.012699  -0.1381\n",
      "  0.57569    0.075356   0.18799   -0.13788   -0.27318   -0.077728\n",
      "  0.084247   0.25097    0.043049  -0.23965    0.24551    0.22808\n",
      "  0.27576    0.042869  -0.34108   -0.51923    0.32937   -0.18645\n",
      " -0.39077   -0.35076   -0.31796    0.13595    0.044108   0.45992\n",
      " -0.81088    0.087567   0.12398    0.28878   -0.061377   0.66974\n",
      "  0.32958    0.24326    0.15403   -0.27307    0.31536   -0.030252\n",
      "  0.038238  -0.27008    0.23027   -0.29331    0.040362   0.1871\n",
      " -0.2015    -0.086936  -0.27199    0.53149   -0.11389    0.59724\n",
      "  0.12557   -0.55128    0.18334   -0.088754  -0.42015    0.57332\n",
      " -0.068918   0.36101    0.046377   0.63716    0.41277    0.25679\n",
      " -0.16487   -0.39867    0.57375   -0.34657   -0.08315    0.47113\n",
      "  0.32481   -0.32277    0.10939    0.13914    0.059365  -0.5209\n",
      "  0.29847    0.6262    -0.39666    0.077796   0.20521    0.19099\n",
      "  0.098512   0.045333  -0.052134   0.48863   -0.45819   -0.055411 ]\n",
      "[ 0.20641   -0.64424   -0.099469  -0.28884    0.27179    0.74694\n",
      " -0.13093    0.53284   -0.10447   -0.76855   -0.010646  -0.22562\n",
      "  0.17798   -0.061048   0.17108    0.65413    0.6563    -0.83955\n",
      "  0.96549    0.43136    0.30184   -0.47953   -0.2677    -0.46162\n",
      " -0.018252   0.30995    0.25036    1.369     -0.91659   -0.23976\n",
      " -0.29476   -0.49224    0.52615    0.3972    -0.41056   -0.30054\n",
      " -0.4563    -0.57018   -0.47454    0.6098     0.10229    0.12278\n",
      "  0.7564    -0.65636    0.23088    0.28063   -0.50881   -0.44786\n",
      "  0.27759   -0.46942   -0.081808   0.046322  -0.43699    0.52308\n",
      "  0.060913  -0.1285    -0.66902   -0.07253   -0.51458    0.20433\n",
      "  0.089903  -0.17541    0.053744  -0.72697    0.49369    0.68467\n",
      "  0.36992    0.53544   -0.33565    0.2354     0.12934   -0.46669\n",
      " -0.49519   -0.087317  -0.34503    0.068783  -0.85325   -0.2996\n",
      " -0.46095   -0.11314   -0.37056    0.40863   -0.20081    0.081618\n",
      " -0.10432    0.50886    1.2397     0.28544   -0.67904    0.19636\n",
      " -0.049819   0.051242   0.27886   -0.47299   -0.0057666  0.42275\n",
      "  0.048522   1.0982     0.2214     0.2916     0.16985   -0.71921\n",
      "  0.034872   0.53664   -0.63156   -1.1325    -0.056389   0.36626\n",
      " -0.40875   -0.79191    0.25098    0.23244    0.029871   0.22595\n",
      " -0.0029326 -0.0925     0.56888   -0.36994   -0.25104   -0.34608\n",
      "  0.33561   -0.065033   0.4324     0.38783    0.069136   0.049766\n",
      "  0.75023    0.1173    -0.44623   -0.50861   -0.027283   0.011633\n",
      "  0.43945   -0.43957    0.61052   -0.70554   -0.065543   0.057456\n",
      " -0.04491   -0.36711    0.25993   -0.36699   -0.59616    0.15594\n",
      " -0.14008    0.59821    0.86503   -0.29764   -0.072514  -0.39285\n",
      "  0.10226   -0.23302   -0.021976   0.19384   -0.0089182  0.27022\n",
      " -0.086759  -0.11896   -0.19047    0.078513   0.15283   -0.66265\n",
      "  0.49715    0.35985    0.56332    0.21215   -0.51179   -0.5251\n",
      " -0.88357   -0.60753   -0.47574   -0.49632   -0.0351     0.14553\n",
      " -0.41731    0.45128    0.097337   0.28069   -0.048021   0.11192\n",
      " -0.21914   -0.48933    0.097768   0.37038   -0.017294  -0.65384\n",
      "  0.17172   -0.5579     0.073618   0.12484    0.43896    0.32682\n",
      "  0.11848   -0.087802   0.15816   -0.22818    0.70267    0.18624\n",
      "  0.16306   -0.036477   0.031847   0.22423   -0.20002    0.1241\n",
      " -0.35136    0.35755    0.17786   -0.015734   0.45563   -0.37939\n",
      "  0.25768   -0.051219  -0.3161     0.20315    0.71711   -0.44125\n",
      "  0.70813   -0.19434    0.60685    0.24121   -0.58745    0.13491\n",
      " -0.76275   -0.44162   -0.78257   -0.85535   -0.35587    0.057797\n",
      " -0.033663  -0.70545    0.37098   -0.096703   0.29232    0.20349\n",
      " -0.7972    -0.024191  -0.29806    0.22762   -0.184      0.084\n",
      " -0.66943    0.57669   -0.25736   -0.12383    0.38707   -0.28279\n",
      "  0.26327    0.67761    0.44406    0.069589   0.63733   -0.64284\n",
      " -0.20482   -0.23328   -0.54091   -0.83178    0.24576   -0.469\n",
      " -0.0018605 -0.20288   -0.10344    0.011475   0.099164  -0.050437\n",
      "  0.51723   -0.28013    0.11144   -0.10701   -0.41013    0.80629\n",
      " -0.61744   -0.067607  -0.18688   -0.10428   -0.43683    0.46506\n",
      " -0.4978    -0.75666    0.38115   -0.43644   -0.48935   -0.41161\n",
      " -0.13335   -0.47081   -0.36429   -0.10811   -0.14647   -0.45426\n",
      "  0.15814    0.23543   -0.70209    0.39548    0.2384    -0.16142\n",
      " -0.75904   -0.31516   -0.39949    0.048385   0.11535    0.28755  ]\n",
      "[ 0.15838    0.23174   -0.39964    0.27989    0.48079    0.13798\n",
      " -0.12312   -0.47204    0.34569   -0.99075    1.0562     0.096654\n",
      "  0.62349   -0.14897    0.63645    0.18607    0.092299  -0.80485\n",
      "  0.44254    0.42491   -0.069963   0.026943   0.46507    0.17794\n",
      " -0.72464    0.041642   0.15752    0.15183   -0.6044     0.043018\n",
      "  1.0204    -0.93734    0.85241    0.012074  -0.54085   -0.66243\n",
      "  0.43701   -0.20464    0.17415    0.30426   -0.1218    -0.53758\n",
      "  0.02869    0.64679    0.58024    0.6232    -0.063702   0.018208\n",
      "  0.19811    0.57473   -0.14379   -0.19173   -0.30659    0.15535\n",
      "  0.37588   -0.016263  -0.82697    0.042002   0.035847  -0.58058\n",
      "  0.51558    0.25476    1.1213    -0.11278    0.91686   -0.45929\n",
      "  0.21405    0.047528  -0.84042   -0.57718    0.011818   0.52671\n",
      " -0.45915    0.60237    0.65948    0.34284    1.1731    -0.73026\n",
      "  0.33107    0.59844   -0.67469    0.50607    0.99236    0.076639\n",
      " -0.62662    0.74385    0.69794   -0.28135   -0.75064   -0.38546\n",
      " -0.63835   -0.040608   0.45644   -0.58062    0.17397    0.57543\n",
      "  0.36311    0.89545   -0.57215   -0.36415    0.0067925  0.11513\n",
      "  0.10463    1.1283    -0.90515   -1.6123     0.092707   0.48955\n",
      " -0.04884    0.363     -0.060924  -0.66326   -0.38787   -0.69818\n",
      " -0.74505    0.61555    0.2585    -0.87997    0.54994    0.024003\n",
      "  0.1346     0.67504   -0.27349   -0.022718  -0.51491    0.26395\n",
      "  0.0063872  0.99376   -0.046857  -0.3298     0.49921   -0.4851\n",
      " -0.11543   -0.45835    0.12137    0.17184    0.15184   -0.25438\n",
      " -0.16219   -1.1168     0.32222   -0.37624   -0.76014   -0.64903\n",
      " -0.032952   1.0073     0.72122   -0.14453   -1.128      0.094782\n",
      "  0.39464    0.22872   -1.0379     0.16392    0.24652    0.53372\n",
      "  0.35841    0.17298    0.54062    0.6199     0.49079    0.22548\n",
      " -0.33996    0.19824    0.65864   -0.39538   -0.031633   0.17841\n",
      " -0.5264    -0.45624   -0.0204    -0.87983    0.39599   -0.33813\n",
      "  1.0324     0.47675    0.74929   -0.47718    0.066194   1.0145\n",
      "  0.47797   -0.59085   -0.19904   -0.36418    0.91768   -0.089035\n",
      " -0.28842   -0.76554   -0.49566    0.16045    0.35131   -0.085809\n",
      " -0.45648   -0.7153     0.36825   -0.54659   -0.15408    0.13001\n",
      "  0.95009   -0.11408   -0.048781  -0.52998   -0.53491   -0.055836\n",
      " -0.41792    0.34617    0.47287   -0.61797    0.46336   -0.033509\n",
      "  0.29237    0.72541    0.60754    0.48957   -0.24531    0.19148\n",
      "  0.79499    0.065433   0.36546   -0.082328  -0.21447    1.4137\n",
      " -0.17965   -0.46843   -0.23269   -0.57747   -0.35092    0.013038\n",
      "  0.48233   -0.65354    0.43373   -0.30989    0.73786   -0.086606\n",
      " -0.30037   -0.72514   -0.035666   0.07558   -0.83272    0.50924\n",
      "  0.36547    0.07351    0.47479    0.19359    0.55501   -0.19805\n",
      "  0.26408   -0.45267    0.57199    0.16042   -0.33684   -0.7246\n",
      " -0.15301    0.25454   -0.33212   -0.40439   -0.51607   -0.71488\n",
      " -0.29621   -0.11932   -0.35284   -0.35994    0.040399   0.17515\n",
      "  0.35859    0.44058   -0.57561   -0.30172    0.5047     0.01172\n",
      " -1.0745     0.24066    0.068681   0.4878    -0.94988   -0.25576\n",
      " -0.28152   -0.10563    1.3583    -0.42567    0.071918  -0.0369\n",
      " -1.1829    -0.546      0.084754   0.36072   -0.18273   -0.10776\n",
      "  0.29468    0.20701    0.04757    0.3923    -0.37591    0.033401\n",
      "  0.34018   -0.35825   -0.32595    1.4007    -0.39519    0.45688  ]\n",
      "[ 1.1605e-01 -5.7126e-01 -4.6880e-01 -2.3793e-02  2.0606e-01 -9.9541e-03\n",
      " -1.4388e-01  8.2657e-01  2.2683e-01 -1.3466e+00  1.5631e-01 -8.3306e-01\n",
      "  3.4255e-01  2.0497e-01 -1.9230e-01 -2.9334e-01 -2.0231e-01 -8.2387e-01\n",
      " -1.1193e-01 -1.0966e-01 -1.3550e-01  9.0708e-01 -9.0382e-02 -1.3925e-01\n",
      " -2.5437e-02  9.6722e-01 -2.5048e-01  3.0881e-01  4.7241e-02  6.9182e-02\n",
      "  7.3767e-01 -1.0864e+00 -1.0599e-01  3.3040e-01  2.1316e-01  1.1327e-01\n",
      " -7.4602e-01  2.5281e-01  3.1894e-01  5.9886e-02 -2.3533e-01 -2.8646e-01\n",
      "  3.4410e-01  2.6688e-01  1.9529e-02 -1.0178e-01 -1.3673e-01  1.1752e-01\n",
      " -4.9320e-01  2.0517e-01 -7.0649e-01  2.0506e-01 -8.8807e-03  4.8047e-01\n",
      "  5.8208e-02  1.7481e-02 -9.7435e-02 -8.1725e-01 -2.1942e-01 -1.1682e+00\n",
      "  2.5241e-01 -2.0807e-01  2.3090e-01 -2.5348e-01  5.0295e-02  5.2983e-01\n",
      "  4.2134e-01 -1.8771e-01 -1.6299e-01 -5.5897e-01 -3.6229e-02  6.3322e-01\n",
      "  4.0250e-01 -8.3858e-02 -5.3944e-01  7.4020e-01  3.4020e-01  3.4643e-01\n",
      " -4.9769e-01  1.4632e-02 -5.1839e-01 -3.0881e-01  1.4945e-01  6.8327e-02\n",
      " -4.7390e-01  7.7948e-01  7.0127e-01 -5.0313e-01 -6.8554e-01 -8.1939e-01\n",
      "  6.6606e-01  9.0875e-01 -4.0975e-01 -5.4323e-01  2.6617e-01 -1.1903e-01\n",
      "  4.0893e-01  3.4201e-01 -1.5260e-01 -1.5571e-01 -4.1438e-01 -4.2089e-01\n",
      " -2.4118e-01  6.4858e-01 -7.9550e-01  2.2605e-02  6.8298e-01  6.3800e-01\n",
      "  6.7782e-01 -3.1303e-01 -9.0769e-02  3.9658e-01  4.5877e-01 -2.1334e-01\n",
      " -6.7774e-01  5.5640e-01 -5.6208e-01 -1.1214e+00 -7.9917e-02 -1.1669e-01\n",
      " -4.2128e-02  7.2943e-04  2.3884e-01  8.6769e-02  3.3328e-01  4.6692e-01\n",
      " -3.6587e-01  9.6553e-02 -5.4690e-01 -7.0563e-01  1.4897e-01 -4.4398e-01\n",
      "  7.9262e-02  3.0742e-01 -3.8485e-01  6.2813e-01 -4.1639e-01  4.3049e-01\n",
      "  2.8310e-01 -2.4888e-01  1.4057e+00 -2.9489e-01 -1.9093e-01 -7.5545e-01\n",
      "  4.1396e-01  2.8861e-01  6.4466e-01  2.9517e-01 -3.2381e-01  1.1666e-02\n",
      "  1.2021e-01  3.0073e-01 -6.3371e-02 -6.4965e-01 -1.9032e-01 -1.9538e-01\n",
      "  9.6025e-01  1.7546e-01  4.8235e-01  2.1508e-01 -2.8766e-01  2.5257e-02\n",
      " -6.4722e-01  4.5608e-01  4.9463e-02  4.8324e-01  5.0834e-01  1.5976e-01\n",
      "  1.0640e-01 -3.2485e-01  4.4625e-02 -4.3400e-01  1.7951e-01 -1.7890e-01\n",
      "  2.9612e-01 -1.0519e+00 -5.4753e-02 -4.7853e-01 -3.7425e-01  1.1478e-01\n",
      " -2.3997e-01 -5.7313e-02  1.3289e-01  3.0044e-01 -4.9886e-01 -1.8454e-01\n",
      "  9.1700e-01  3.3607e-01 -3.6420e-01 -1.3715e-01  4.1861e-01 -3.3978e-01\n",
      " -2.7991e-01 -1.8986e-01 -1.3588e-01 -2.2827e-01  6.5313e-01 -9.4339e-02\n",
      " -1.9699e-01 -8.9950e-01 -3.3226e-01  1.3634e-01 -3.9242e-01 -5.5414e-01\n",
      " -2.6963e-01 -1.0919e+00  4.1230e-01 -4.6141e-01 -2.5375e-01  4.4717e-02\n",
      "  1.1695e-01  4.7870e-02  3.8078e-01 -8.5441e-03  2.7587e-01 -2.2239e-01\n",
      "  3.5062e-01 -3.7872e-01  9.5123e-01 -2.3577e-01  4.4087e-01  1.8745e-02\n",
      " -1.8433e-01 -3.2337e-01 -1.2350e-01 -6.4806e-01  4.4698e-01  1.4802e-01\n",
      "  2.0899e-02  3.9093e-02  2.4995e-01  3.2240e-01  3.4107e-01  5.2582e-02\n",
      "  4.3854e-01 -4.5038e-01  1.8709e-01 -1.5344e-01  3.8973e-01 -1.3758e-01\n",
      " -7.0789e-01 -1.9571e-01  6.1092e-01 -2.5439e-01  4.8153e-01 -5.7470e-01\n",
      " -2.5753e-01 -4.2434e-01  1.8876e-01  3.4036e-01  4.5749e-02 -2.8349e-01\n",
      " -3.4631e-01  5.5318e-02 -1.5522e-01 -1.6144e+00 -1.1679e-01  3.0495e-01\n",
      "  5.0907e-01  4.3392e-01 -3.5869e-02  8.5196e-02 -2.0547e-02  7.1519e-01\n",
      "  1.1434e-01  2.8708e-01 -3.0763e-01  2.9075e-01 -4.2046e-02 -5.4872e-01\n",
      " -2.5269e-01 -5.2511e-01  6.2948e-01  6.4780e-01 -4.9105e-01  3.7672e-01\n",
      " -1.5678e-01  7.3838e-02 -3.0550e-01 -5.8028e-01 -1.4742e-02 -9.4933e-03\n",
      "  4.2838e-01  3.4279e-01  1.0551e-01  1.0622e+00 -1.1290e-02 -1.1620e-01\n",
      " -3.6148e-01  2.7754e-01  2.2775e-01  7.2573e-01 -1.7193e-01 -3.1974e-01\n",
      " -5.5951e-01  6.1713e-01 -2.4774e-01  3.4741e-01  1.8426e-01  2.9359e-01]\n"
     ]
    }
   ],
   "source": [
    "print (glove_embeddings['Schwab'])\n",
    "print(glove_embeddings['rmd'])\n",
    "print(glove_embeddings['vanguard'])\n",
    "print(glove_embeddings['blackrock'])\n",
    "print(glove_embeddings['ameritrade'])\n",
    "print(glove_embeddings['SCHW'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.799e-01 -6.090e-02  3.940e-02  1.563e-01 -1.152e-01 -7.056e-01\n",
      "  3.599e-01 -3.455e-01 -5.410e-01  3.297e-01 -2.468e-01  3.851e-01\n",
      " -4.890e-02  3.203e-01 -9.200e-03  4.183e-01  1.799e-01 -1.656e-01\n",
      " -1.567e-01  1.897e-01 -1.128e-01  2.519e-01 -7.587e-01  2.604e-01\n",
      "  1.763e-01  3.257e-01  8.870e-02 -3.518e-01 -6.119e-01  1.431e-01\n",
      "  4.926e-01  1.298e-01 -2.824e-01  2.730e-01  3.544e-01  1.392e-01\n",
      " -3.062e-01  5.110e-02 -1.973e-01 -2.703e-01  9.570e-02 -1.982e-01\n",
      " -4.788e-01  6.519e-01  2.258e-01 -4.140e-01 -2.274e-01  3.050e-02\n",
      "  3.159e-01 -3.560e-01  1.755e-01 -2.430e-01  6.916e-01  4.952e-01\n",
      " -2.976e-01  4.380e-01 -2.934e-01  1.485e-01 -2.430e-02 -2.160e-02\n",
      " -4.477e-01 -1.428e-01  2.560e-01  8.540e-02  4.970e-02  2.503e-01\n",
      "  3.531e-01  1.051e-01  8.440e-02  1.166e-01 -3.892e-01  7.640e-02\n",
      "  1.361e-01 -4.056e-01 -4.945e-01 -1.586e-01 -4.023e-01  2.919e-01\n",
      " -3.220e-01 -2.810e-02  4.450e-01 -9.910e-02 -1.145e-01 -2.313e-01\n",
      " -4.943e-01 -5.789e-01 -3.248e-01  2.097e-01  2.743e-01 -2.104e-01\n",
      " -4.820e-02  4.175e-01  4.580e-02 -3.038e-01  3.301e-01 -5.120e-02\n",
      " -3.674e-01  6.010e-02  1.699e-01 -2.581e-01  3.798e-01 -4.144e-01\n",
      "  3.320e-01 -2.017e-01  1.131e-01  4.494e-01  2.490e-02  8.610e-02\n",
      " -4.394e-01  1.821e-01 -8.730e-02  6.600e-02  6.990e-02  1.994e-01\n",
      " -9.057e-01 -2.150e-01 -2.037e-01  1.318e-01  7.750e-02 -1.176e-01\n",
      " -3.230e-02  3.226e-01 -4.300e-02 -4.208e-01  6.090e-02  3.760e-02\n",
      " -1.081e-01  2.501e-01  2.266e-01 -9.140e-02  1.273e-01 -8.100e-03\n",
      " -1.426e-01 -9.000e-03  3.212e-01 -4.490e-02 -6.210e-02 -1.559e-01\n",
      "  1.623e-01 -5.223e-01  6.180e-02 -3.358e-01  1.676e-01 -2.192e-01\n",
      " -2.245e-01 -1.512e-01  3.756e-01  1.417e-01 -2.078e-01  2.523e-01\n",
      " -1.402e-01 -1.565e-01 -1.901e-01  7.131e-01  4.048e-01 -5.450e-02\n",
      " -1.556e-01 -4.342e-01 -5.032e-01 -1.800e-02  6.097e-01 -3.910e-02\n",
      "  1.710e-02 -1.356e-01 -9.680e-02 -1.170e-01  3.467e-01 -2.420e-02\n",
      " -3.146e-01 -4.041e-01  3.087e-01  2.711e-01 -3.666e-01  4.080e-02\n",
      "  3.214e-01  4.158e-01  1.300e-03  2.020e-02  9.970e-02 -8.700e-02\n",
      " -3.154e-01  3.310e-02 -2.975e-01  1.287e-01 -9.180e-02 -1.203e-01\n",
      "  2.436e-01  2.864e-01 -1.958e-01  1.648e-01  2.116e-01 -1.441e-01\n",
      " -8.040e-02 -3.465e-01 -4.463e-01 -1.006e-01  1.891e-01  3.000e-04\n",
      " -1.053e-01  3.073e-01  1.693e-01 -1.542e-01 -2.125e-01 -2.992e-01\n",
      " -1.030e-02  2.169e-01 -2.528e-01 -6.277e-01  2.159e-01 -2.223e-01\n",
      " -3.744e-01  1.250e-02  3.125e-01  7.630e-02  1.405e-01 -4.137e-01\n",
      " -4.507e-01 -2.697e-01 -4.960e-02 -2.858e-01  3.535e-01  5.732e-01\n",
      "  4.019e-01 -8.950e-02 -1.241e-01  3.023e-01 -1.182e-01 -1.061e-01\n",
      " -1.857e-01  2.390e-02 -4.840e-02  2.162e-01 -9.570e-02 -3.769e-01\n",
      "  8.200e-03  1.650e-02 -6.160e-02  4.063e-01 -4.698e-01 -6.830e-02\n",
      "  1.866e-01  1.010e-01  7.620e-02  4.545e-01  2.016e-01  1.518e-01\n",
      " -3.743e-01 -1.748e-01 -4.261e-01  2.397e-01 -2.617e-01 -1.137e-01\n",
      "  3.796e-01 -1.461e-01 -2.937e-01 -1.217e-01 -2.077e-01 -5.243e-01\n",
      " -3.442e-01 -4.367e-01 -6.550e-02 -6.397e-01  2.333e-01  6.440e-02\n",
      "  7.000e-02  2.072e-01  2.743e-01 -1.907e-01 -1.475e-01 -1.840e-02\n",
      " -6.600e-02 -2.166e-01  2.216e-01 -1.270e-01  8.680e-02  5.070e-02\n",
      "  6.680e-02 -1.662e-01  1.750e-01 -6.700e-02  4.800e-03 -4.730e-02\n",
      "  3.418e-01 -1.380e-02  2.799e-01  1.421e-01  1.810e-02  3.183e-01\n",
      "  7.130e-02  4.380e-02 -1.614e-01 -8.084e-01  3.110e-02  2.152e-01\n",
      "  2.997e-01  2.914e-01  3.011e-01 -9.210e-02 -1.427e-01 -2.934e-01]\n",
      "[-2.1170e-01 -1.7830e-01 -4.7410e-01  4.1530e-01 -1.2100e-02 -2.6060e-01\n",
      " -1.3500e-01 -2.3580e-01  6.0600e-02  1.7860e-01  2.5900e-02  1.3357e+00\n",
      " -2.0500e-01 -2.2920e-01  1.3490e-01  2.4520e-01  3.8850e-01 -4.8710e-01\n",
      " -4.9170e-01  6.3400e-01 -1.1810e-01  4.8200e-02 -3.8010e-01  4.7740e-01\n",
      "  6.1000e-03  2.1130e-01 -1.6960e-01 -2.5620e-01 -4.8760e-01 -1.0970e-01\n",
      "  1.5500e-02 -1.2120e-01  1.4890e-01  1.1010e-01 -9.6400e-02  3.9570e-01\n",
      "  1.2970e-01 -5.8000e-02 -1.2800e-01 -8.4400e-02 -5.5380e-01  3.1500e-02\n",
      " -4.9010e-01  2.9870e-01  5.5700e-02 -3.0140e-01 -1.5900e-01  4.0200e-02\n",
      "  3.3680e-01  9.6100e-02  9.9500e-02  1.3970e-01  1.0770e+00 -7.9800e-02\n",
      "  1.0800e-01 -4.7500e-02 -6.1320e-01 -1.9650e-01 -7.8000e-02  7.0900e-02\n",
      "  1.7660e-01  2.4120e-01 -7.6010e-01 -1.4300e-01 -6.2800e-01 -3.9470e-01\n",
      "  4.5200e-01  3.6700e-02 -1.8610e-01 -2.5420e-01 -2.4910e-01  1.0800e-01\n",
      "  4.4940e-01 -5.6750e-01  5.7300e-02  2.6510e-01 -2.2100e-01  6.0500e-02\n",
      " -1.8230e-01 -3.7130e-01  5.5540e-01 -4.0440e-01 -1.3700e-02 -3.8890e-01\n",
      " -2.2470e-01 -2.8560e-01  1.5650e-01  8.9100e-02  1.0330e-01  3.0200e-02\n",
      " -4.3020e-01  1.9150e-01  2.9650e-01  2.5560e-01  2.2330e-01 -3.1100e-01\n",
      "  6.3600e-02 -6.2290e-01 -3.1980e-01 -1.1220e-01 -4.4220e-01 -5.0100e-02\n",
      "  1.7900e-02 -1.7580e-01 -2.3830e-01 -3.2140e-01  2.6110e-01 -3.0840e-01\n",
      "  4.4870e-01  5.8430e-01 -4.2820e-01 -3.7460e-01  6.9350e-01  5.5800e-02\n",
      " -5.5900e-01 -8.2200e-02  1.1000e-02  5.0990e-01 -5.4080e-01  2.7690e-01\n",
      " -7.8060e-01  3.1940e-01  7.2500e-02  5.5270e-01 -4.7410e-01 -3.7590e-01\n",
      " -6.3700e-02 -7.0000e-04  1.6170e-01 -4.7080e-01  3.7190e-01  4.3950e-01\n",
      " -8.0300e-02  4.6200e-01 -1.0680e-01 -4.2870e-01 -1.0570e-01 -2.2000e-01\n",
      "  3.4740e-01 -6.6860e-01 -2.7500e-01  5.7890e-01 -5.4050e-01 -9.2200e-02\n",
      "  2.6000e-03  5.1440e-01  1.9820e-01 -7.3930e-01 -7.8320e-01  4.1530e-01\n",
      "  6.5350e-01 -3.1140e-01 -1.3590e-01 -2.9100e-02  1.9420e-01  5.1250e-01\n",
      "  2.7660e-01 -3.3650e-01 -8.2990e-01  4.4000e-03  2.5480e-01 -2.3760e-01\n",
      "  4.4300e-02 -3.4100e-02 -2.7840e-01 -2.3620e-01  7.5300e-02  7.3800e-02\n",
      " -8.8730e-01  3.6020e-01  6.8700e-02  1.1840e-01  9.0400e-02  1.1040e-01\n",
      " -3.1450e-01  5.3670e-01  4.0000e-03 -1.8600e-01  4.6800e-02  4.2860e-01\n",
      "  1.1900e-02 -4.1820e-01  5.9220e-01  2.7670e-01 -1.9750e-01 -9.5400e-02\n",
      " -1.9900e-01 -1.0900e-02 -4.1570e-01 -8.7000e-02  2.2480e-01 -2.5190e-01\n",
      " -1.1830e-01  3.6800e-01 -5.5690e-01 -2.8660e-01  7.7400e-02  5.3070e-01\n",
      "  1.7390e-01 -2.0480e-01 -2.8710e-01 -5.9000e-02  3.6300e-01 -2.2090e-01\n",
      " -3.4610e-01  3.0130e-01 -5.7340e-01  4.5000e-01  1.8110e-01 -1.2520e-01\n",
      " -2.0030e-01  3.7520e-01 -3.2340e-01  1.2940e-01 -2.0600e-01  1.0400e-01\n",
      " -8.7600e-02  2.0120e-01 -4.2870e-01 -4.5940e-01  7.0900e-02 -3.7700e-01\n",
      "  6.9910e-01 -2.5830e-01  5.1890e-01  3.4220e-01 -3.3000e-03 -1.1170e-01\n",
      " -1.5760e-01 -6.9000e-02 -2.6740e-01 -1.5450e-01 -1.1230e-01 -5.1580e-01\n",
      " -5.0070e-01 -6.7080e-01  6.2960e-01  7.1360e-01  1.3000e-03 -2.3780e-01\n",
      " -2.5400e-01  2.8690e-01 -9.0500e-02  3.2350e-01 -3.1730e-01  4.8620e-01\n",
      " -1.8200e-02 -1.0800e-02 -3.0740e-01  4.2310e-01  5.6080e-01 -2.9900e-02\n",
      " -4.8240e-01  7.4900e-02  7.4700e-02  2.1390e-01  1.3010e-01  6.9690e-01\n",
      "  4.0030e-01 -6.4090e-01 -3.6490e-01  1.1990e-01  3.7840e-01  3.7600e-02\n",
      "  5.9480e-01  1.3490e-01  4.7920e-01 -1.9800e-02 -1.6950e-01 -3.1000e-03\n",
      " -1.3280e-01  1.6600e-01 -5.0870e-01  5.4120e-01 -9.6800e-02  2.0900e-01\n",
      "  5.0100e-02 -6.0400e-02 -2.9860e-01  1.7580e-01 -1.0040e-01  1.1100e-01\n",
      "  3.8860e-01 -1.2450e-01 -6.4710e-01  1.9440e-01 -5.7200e-02  1.8280e-01\n",
      " -2.9600e-01  1.0050e-01  5.2100e-01  3.1410e-01  1.7290e-01  8.7900e-02\n",
      "  9.4880e-01  3.7790e-01  1.1370e-01 -2.4300e-02 -2.5600e-02 -7.9140e-01]\n",
      "[-0.0217 -0.1136 -0.0887  0.1121 -0.2469 -0.131  -0.4134 -0.2268  0.2426\n",
      "  0.1078 -0.0858  0.3679  0.2611  0.3497  0.1602  0.0949  0.0359 -0.0153\n",
      "  0.2751 -0.2709  0.1178  0.2604 -0.2202  0.0744 -0.2622 -0.0311 -0.6913\n",
      "  0.0429 -0.354  -0.2722  0.0677  0.0427 -0.191   0.0095 -0.1895 -0.1675\n",
      "  0.2066 -0.2594  0.2237 -0.0275  0.5442 -0.1169 -0.2843 -0.053   0.2504\n",
      "  0.0252 -0.0936 -0.0109  0.0913  0.1003 -0.0507 -0.214  -0.2542 -0.4158\n",
      "  0.2364  0.0675  0.1503  0.      0.3838  0.4412  0.1714  0.1243  0.1105\n",
      "  0.4114  0.103   0.4802 -0.1856  0.1059  0.2596  0.0882  0.1411 -0.3452\n",
      "  0.0903  0.0774 -0.2255 -0.0834 -0.4698 -0.4485  0.2421  0.4972 -0.0044\n",
      " -0.2642 -0.4844 -0.2398  0.5005 -0.0755 -0.0044  0.1291  0.1664  0.1119\n",
      "  0.088  -0.0173 -0.4754 -0.107   0.0777 -0.1976  0.1687 -0.0311 -0.0103\n",
      " -0.1035  0.066  -0.2655  0.1367 -0.1891 -0.2939  0.103  -0.001  -0.0451\n",
      "  0.2894 -0.3797  0.1179  0.1518 -0.1181  0.1036 -0.5622 -0.3205 -0.1497\n",
      "  0.2203  0.589   0.0313 -0.1024  0.3767  0.2241  0.1342 -0.5037 -0.0811\n",
      " -0.4795  0.8492  0.1557  0.0126 -0.0844  0.0869  0.1727  0.158   0.1795\n",
      "  0.1934 -0.2    -0.2254 -0.0889 -0.549   0.0219  0.0447 -0.0328 -0.1328\n",
      " -0.1346 -0.0462 -0.2003 -0.0383 -0.0934  0.2654 -0.1019 -0.0473 -0.3045\n",
      " -0.2956  0.6253  0.1193 -0.0412 -0.1504 -0.38    0.3231  0.3837 -0.2861\n",
      "  0.169   0.6154 -0.1372 -0.2174  0.0609  0.0535  0.0994 -0.2146  0.1738\n",
      "  0.2238 -0.2154 -0.0676 -0.143  -0.1621  0.0032  0.1158  0.2392  0.1953\n",
      "  0.1611  0.1672 -0.053  -0.3458 -0.4073 -0.1922 -0.2732  0.3713  0.0318\n",
      " -0.2375  0.1934 -0.2237 -0.3502  0.3401  0.2617 -0.0644 -0.1399  0.0885\n",
      " -0.4712  0.2629  0.0656 -0.1288 -0.1732  0.2326 -0.2157 -0.1367 -0.059\n",
      " -0.0741  0.0235  0.0821 -0.0147  0.6144  0.1007 -0.2186  0.166  -0.1606\n",
      " -0.0677  0.0262 -0.0403 -0.2882 -0.4071  0.3408 -0.3053 -0.3333 -0.0013\n",
      "  0.1676 -0.0291 -0.4201  0.0041  0.2491 -0.4235  0.0152 -0.1179 -0.0837\n",
      "  0.4924 -0.8097 -0.207  -0.0863  0.1418  0.0182  0.104   0.0659  0.0504\n",
      " -0.4873 -0.1375  0.2704  0.4253  0.2895  0.0819  0.1215  0.0107  0.3455\n",
      " -0.0036  0.4049  0.1167 -0.1496  0.1318 -0.0908 -0.0387 -0.1483 -0.1071\n",
      "  0.0859  0.6567  0.0826  0.141  -0.1799  0.2578  0.1651 -0.1553 -0.0393\n",
      " -0.4398  0.2724 -0.2083  0.2718 -0.0202 -0.1759 -0.0949 -0.06    0.0515\n",
      " -0.056  -0.0323  0.2509 -0.0222 -0.0038  0.2113  0.3482  0.1768  0.2371\n",
      " -0.1901 -0.0877 -0.1078 -0.3546  0.1931  0.064   0.4539 -0.2765 -0.3922\n",
      " -0.2438 -0.2776 -0.1538]\n",
      "[-2.9900e-02 -7.2500e-02 -4.2420e-01  1.5430e-01 -1.0960e-01 -5.1550e-01\n",
      " -5.1390e-01  1.2020e-01  1.8900e-02  6.4480e-01 -1.3480e-01  1.3410e+00\n",
      " -6.5740e-01 -6.0200e-02  3.7480e-01 -4.2300e-02 -2.5200e-02  1.6940e-01\n",
      " -1.8000e-02 -8.5800e-02 -2.4560e-01  3.4000e-01 -4.1230e-01 -2.2300e-02\n",
      " -3.3110e-01 -1.9030e-01 -1.4590e-01  5.4520e-01 -5.8710e-01 -5.1060e-01\n",
      "  6.0330e-01  4.7470e-01 -7.7500e-02 -2.7470e-01  2.4720e-01 -3.3910e-01\n",
      " -2.8640e-01  7.7200e-02  1.6490e-01 -3.2400e-02 -7.7200e-02 -2.3910e-01\n",
      " -9.0000e-02  1.9000e-03  3.2930e-01 -3.0850e-01  1.2100e-02 -1.7440e-01\n",
      " -4.8300e-02 -3.2190e-01 -3.1900e-01  3.6030e-01  8.8230e-01  8.8000e-01\n",
      "  7.4540e-01  1.7850e-01 -2.9090e-01 -1.8870e-01 -2.1490e-01  9.2000e-02\n",
      "  1.0020e-01 -4.5880e-01  2.5410e-01 -5.3700e-02  1.3700e-02  4.5200e-02\n",
      "  1.1020e-01 -6.4200e-02  4.3210e-01 -9.2400e-02 -6.6700e-02 -3.8080e-01\n",
      "  4.6840e-01 -6.3400e-02 -5.5950e-01  4.7540e-01 -3.6290e-01  9.0900e-02\n",
      " -2.1700e-02 -9.6500e-02  4.3540e-01  7.1800e-02  4.9860e-01  5.7800e-02\n",
      "  6.4240e-01 -1.5990e-01 -2.6950e-01  1.0540e-01  7.6810e-01  2.7070e-01\n",
      " -1.8590e-01  5.1400e-02  2.1970e-01 -5.6300e-02  8.1030e-01  3.1000e-02\n",
      "  1.9900e-01 -3.8920e-01  3.0040e-01  6.7100e-02 -5.3290e-01 -5.5330e-01\n",
      " -1.6490e-01  5.9000e-03  2.7900e-02  2.5560e-01 -5.8920e-01 -6.8950e-01\n",
      "  7.7200e-02 -1.9940e-01 -2.8650e-01 -3.3500e-02  3.7720e-01  2.8970e-01\n",
      " -9.0950e-01  4.8660e-01  1.9530e-01 -3.7150e-01 -5.3500e-02  5.4530e-01\n",
      "  2.3240e-01  7.6340e-01 -5.3520e-01  9.4300e-02  4.9900e-02 -6.1160e-01\n",
      " -6.7350e-01  2.0200e-01  7.7200e-02 -4.4200e-01 -1.8000e-02  9.5600e-02\n",
      "  6.9600e-02  1.2080e-01  1.0230e-01  6.5180e-01  1.3800e-01  1.0500e-02\n",
      "  5.3870e-01 -6.9430e-01 -2.7030e-01  1.3870e-01  8.0910e-01  3.2370e-01\n",
      " -9.9300e-01 -9.1900e-02 -3.1530e-01 -5.9890e-01  2.1450e-01  4.5400e-02\n",
      "  2.2990e-01  4.6030e-01 -1.9500e-02  3.4380e-01  6.6020e-01 -3.2580e-01\n",
      "  7.4500e-02 -7.1100e-02  2.9200e-02  2.8000e-02  1.0440e-01 -4.4600e-02\n",
      "  2.0590e-01 -7.9300e-02  8.0400e-02 -7.7560e-01 -6.9000e-02  3.4200e-02\n",
      " -5.3650e-01 -5.5690e-01  3.8850e-01  6.8650e-01  4.3320e-01  2.7100e-01\n",
      "  1.7520e-01 -4.6500e-02  1.3460e-01  1.0910e-01 -8.8270e-01  2.5000e-02\n",
      " -8.7000e-03 -3.0060e-01 -6.2170e-01 -1.6850e-01 -6.6740e-01 -2.2790e-01\n",
      " -1.0420e-01  4.6210e-01 -4.9500e-02  8.5600e-02 -9.0000e-04  1.1350e-01\n",
      " -3.2770e-01  1.1170e+00 -1.1200e-01  1.9580e-01  7.5070e-01  2.1810e-01\n",
      "  8.1500e-02  1.0329e+00 -2.0500e-02  1.0800e-01  7.8200e-02 -3.3510e-01\n",
      " -4.1820e-01 -7.2660e-01  7.2100e-02 -1.2950e-01  4.6000e-03 -4.2000e-02\n",
      " -4.3400e-01  4.8830e-01 -3.8600e-01  5.7510e-01  1.0580e-01  6.8100e-01\n",
      "  3.6400e-02  1.4000e-01  6.0130e-01  7.9800e-02 -5.0240e-01  3.6010e-01\n",
      " -7.7500e-02 -3.2750e-01  4.7650e-01  6.5480e-01  2.3650e-01 -2.0720e-01\n",
      "  9.3400e-02  6.9630e-01 -1.2690e-01  3.2280e-01 -1.8770e-01 -7.6080e-01\n",
      "  1.8940e-01 -5.7200e-01 -2.0400e-02  4.1600e-02  5.5210e-01 -1.5900e-02\n",
      " -2.0800e-02 -3.2730e-01 -8.4000e-03 -4.0140e-01  1.0760e-01 -3.7860e-01\n",
      "  3.0350e-01 -3.0520e-01 -4.2740e-01  5.1320e-01  3.3300e-01  3.6000e-01\n",
      " -5.9400e-02  6.9070e-01  3.0920e-01  1.3302e+00  1.9100e-02  5.7380e-01\n",
      "  3.7970e-01  1.5980e-01 -7.0790e-01  6.5910e-01 -8.5040e-01 -1.2070e-01\n",
      "  2.5780e-01 -1.4800e-01  1.9480e-01 -4.0140e-01 -1.7500e-02  4.7380e-01\n",
      "  7.8600e-02  2.8120e-01  2.8280e-01  4.0290e-01  2.2230e-01 -1.4860e-01\n",
      "  1.6940e-01 -9.4900e-02 -1.3870e-01  4.4700e-02  3.4620e-01 -2.3380e-01\n",
      " -3.9400e-01 -5.5140e-01 -6.1640e-01  4.1520e-01  3.9800e-02  1.9400e-02\n",
      "  2.4920e-01  2.9050e-01  3.1920e-01 -3.1950e-01 -9.7230e-01  3.2730e-01\n",
      " -1.0650e-01  4.7190e-01 -3.4110e-01 -6.1530e-01  3.2300e-01  2.1200e-01]\n",
      "[ 2.1420e-01  3.3550e-01 -1.7050e-01  8.4700e-02 -6.5900e-02 -1.7150e-01\n",
      "  4.5640e-01  9.0000e-02 -1.5660e-01  1.5660e-01  9.6200e-02  1.3207e+00\n",
      " -7.4100e-02  2.9800e-02  4.3830e-01 -3.8400e-01 -1.1300e-02 -6.3400e-01\n",
      " -1.5530e-01  1.0560e-01 -5.9700e-02  4.6980e-01  1.6830e-01  6.2430e-01\n",
      " -1.3020e-01 -2.9980e-01  4.6950e-01 -3.3000e-02 -7.2690e-01 -3.6930e-01\n",
      " -1.2200e-01  6.4040e-01 -3.0080e-01 -1.9750e-01  4.1090e-01  3.4340e-01\n",
      " -7.2800e-02 -8.7600e-02  5.7940e-01 -9.4000e-03  1.6100e-01  3.9320e-01\n",
      " -1.5350e-01  7.9400e-02  4.5730e-01 -4.8960e-01 -5.4030e-01  8.4500e-02\n",
      " -4.0540e-01  7.6000e-02  4.1100e-01 -1.6630e-01  1.3542e+00 -6.2100e-02\n",
      " -1.8840e-01  5.7800e-02 -2.5790e-01 -5.0130e-01 -3.9200e-02 -3.4510e-01\n",
      " -7.0690e-01 -9.9700e-02 -3.1900e-02 -1.8020e-01 -3.2320e-01 -2.0500e-02\n",
      "  1.4650e-01  1.8440e-01 -1.8990e-01 -1.7900e-01  1.6630e-01  2.7010e-01\n",
      "  5.9460e-01  1.2430e-01 -6.9410e-01  8.9820e-01 -7.6700e-02 -2.7770e-01\n",
      " -2.5790e-01  7.9000e-02  2.8370e-01  4.9500e-02  9.3600e-02 -3.3250e-01\n",
      "  8.6100e-02 -5.9680e-01 -2.1010e-01  4.7600e-02 -3.0590e-01 -6.7600e-02\n",
      "  5.0700e-02  2.1210e-01  3.9670e-01  4.5970e-01 -2.2230e-01  1.0600e-02\n",
      " -5.5300e-02 -2.0340e-01 -1.5650e-01 -1.6630e-01  4.3440e-01 -6.3940e-01\n",
      " -2.2000e-01 -4.5490e-01  1.5360e-01  3.0970e-01 -7.5700e-02 -2.1850e-01\n",
      "  2.4000e-03 -5.4620e-01 -4.6480e-01 -2.0870e-01  2.6690e-01  9.2300e-02\n",
      " -7.1190e-01  4.0500e-02 -2.0990e-01 -1.5330e-01  1.5460e-01  6.1390e-01\n",
      "  1.1710e-01  5.2770e-01  3.0320e-01 -1.0850e-01 -6.1140e-01 -3.1830e-01\n",
      " -5.7970e-01  4.8970e-01  4.0450e-01 -3.4720e-01  4.9100e-02  2.7500e-01\n",
      " -1.8310e-01 -2.4850e-01 -1.6900e-02 -1.0470e-01 -2.5550e-01  7.0400e-02\n",
      "  3.0810e-01 -6.8070e-01 -4.6220e-01 -2.8600e-02  6.8230e-01 -3.5880e-01\n",
      "  3.2320e-01 -2.5830e-01 -4.0350e-01 -1.4580e-01 -4.0910e-01  2.7960e-01\n",
      " -1.9100e-02 -3.9970e-01  5.2950e-01  4.4640e-01  1.2185e+00 -5.4400e-01\n",
      " -7.2800e-02 -7.6720e-01  8.1400e-02 -4.0970e-01  5.8440e-01 -7.7820e-01\n",
      "  9.6000e-02  1.3680e-01  2.9000e-02 -1.6080e-01  7.0650e-01 -1.7380e-01\n",
      "  1.1710e-01 -2.4920e-01  4.8380e-01  7.8730e-01  2.5900e-02  2.2140e-01\n",
      " -4.2020e-01  2.7050e-01  3.0920e-01 -1.9060e-01  6.2230e-01 -2.1780e-01\n",
      " -1.1630e-01 -1.1900e-01 -2.3670e-01  6.7280e-01  2.0480e-01  4.7920e-01\n",
      " -2.4770e-01  2.7270e-01 -1.1480e-01  7.5300e-02  4.9440e-01 -1.9920e-01\n",
      "  3.9900e-01  2.2690e-01 -1.7860e-01 -3.2020e-01  3.2840e-01 -8.2200e-02\n",
      " -4.9790e-01  5.7280e-01  2.8510e-01 -2.4440e-01 -1.0600e-02 -2.6700e-01\n",
      "  1.4110e-01  1.4200e-02  3.9770e-01 -5.4830e-01  2.4680e-01 -4.9690e-01\n",
      " -4.0420e-01  5.5150e-01 -2.7110e-01  3.7930e-01 -3.9090e-01  6.1410e-01\n",
      " -1.4580e-01  5.8450e-01  1.7700e-02  1.3670e-01  1.8110e-01 -1.4330e-01\n",
      "  3.9630e-01 -1.7600e-01  3.3810e-01  9.1090e-01 -3.8590e-01  4.7200e-02\n",
      "  5.7300e-02  3.3560e-01  2.4550e-01 -1.2000e-03  2.7370e-01 -8.9000e-01\n",
      " -3.2710e-01 -7.1820e-01  3.2660e-01 -1.4400e-02 -2.3520e-01  2.4200e-02\n",
      "  3.4920e-01 -2.3950e-01  4.2980e-01  3.1140e-01  1.8700e-02 -2.2800e-02\n",
      "  2.4920e-01  5.5200e-02 -3.1400e-02  4.8760e-01 -3.7500e-02 -2.8310e-01\n",
      "  7.6960e-01 -5.7700e-02  2.5700e-02  9.5310e-01 -3.3930e-01  2.7810e-01\n",
      " -1.8350e-01 -2.7300e-02  1.3000e-03 -4.6750e-01 -2.5990e-01  9.2500e-02\n",
      "  2.4730e-01 -6.9000e-02  6.7610e-01  2.8910e-01 -7.1030e-01 -1.4910e-01\n",
      "  3.9020e-01  1.3450e-01 -2.0000e-04  1.5050e-01  8.7000e-02 -1.3140e-01\n",
      "  4.7700e-02 -8.1000e-02 -1.4640e-01  5.6370e-01 -3.0730e-01 -4.5040e-01\n",
      " -1.9890e-01 -1.7190e-01 -1.4560e-01  9.2210e-01  2.2320e-01  8.5200e-01\n",
      "  2.0460e-01  3.5080e-01  1.2280e-01 -3.5000e-02  1.0250e-01  8.9900e-02\n",
      "  7.0950e-01  3.8440e-01 -3.9040e-01 -3.5000e-02 -3.0480e-01 -1.0866e+00]\n",
      "[-0.4727 -0.1143  0.24    0.0415  0.5187 -0.7425  0.294   0.1904 -0.2042\n",
      "  0.3442 -0.1874  1.1443 -0.0984  0.1378  0.3845  0.601   0.2267 -0.339\n",
      "  0.4182  0.4124 -0.3657 -0.0828 -0.2969  0.5439  0.4079 -0.0657 -0.1541\n",
      " -0.1781 -0.6636 -0.0586 -0.2668  0.8199 -0.1657  0.3083  0.5697 -0.0786\n",
      " -0.3783 -0.1783  0.1797 -0.4335  0.2279  0.3785 -0.6716  0.0717 -0.2387\n",
      " -0.602   0.1794 -0.3397 -0.2674 -0.2549 -0.1239  0.16    0.7467  0.2759\n",
      " -0.784   0.676  -0.1014  0.0303  0.122   0.3052 -0.4556  0.074   0.5334\n",
      " -0.08   -0.4602  0.4953 -0.0211 -0.0515 -0.2379  0.0529 -0.1804  0.0672\n",
      "  0.2364  0.0475 -0.2175  0.3199 -0.3353 -0.0938  0.0468  0.5104  0.3789\n",
      "  0.1855 -0.033   0.0639 -0.7644 -0.7489 -0.4551  0.0293 -0.1241 -0.3904\n",
      " -0.2916  0.236   0.0361  0.201   0.5674  0.1541 -0.2455 -0.4406  0.2815\n",
      "  0.1794 -0.3205  0.2003  0.0457 -0.1166 -0.187   0.5901 -0.0538 -0.1606\n",
      "  0.0896 -0.558  -0.1037  0.0184 -0.3771 -0.1753 -0.6061 -0.6603 -0.126\n",
      " -0.2718 -0.0899  0.813  -0.4457  0.2789  0.2537  0.0253 -0.4082 -0.0834\n",
      " -0.318  -0.0106  0.2205  0.1873 -0.0077  0.1781  0.191  -0.0823 -0.5827\n",
      " -0.068   0.1312  0.1001 -0.6868 -0.95   -0.0605 -0.024   0.0106 -0.6234\n",
      " -0.3565 -0.2703  0.004   0.4332 -0.3955  0.0507 -0.2922 -0.4037 -0.2085\n",
      " -0.2127  0.3732  0.007  -0.0995 -0.2845 -0.0999 -0.3429  0.9658 -0.668\n",
      " -0.4039  0.6779 -0.0043 -0.6007  0.3942 -0.0411  0.1773 -0.622   0.3158\n",
      "  0.3601  0.0955 -0.1748  0.1898  0.5699  0.1178 -0.3397 -0.03    0.2395\n",
      " -0.0983  0.3517 -0.5922 -0.192   0.2149  0.5111 -0.2725  0.1096  0.3428\n",
      "  0.0219 -0.1012 -0.2624  0.1956 -0.1389 -0.1075  0.1428 -0.3017  0.1154\n",
      " -0.6049  0.0411  0.2234 -0.5154  0.3398 -0.4119  0.2926 -0.0941 -0.0474\n",
      " -0.2023  0.2325 -0.5885 -0.4577  0.5248  0.0802  0.4843 -0.0616  0.0371\n",
      " -0.2891  0.2172 -0.1189 -0.9551  0.4482  0.0109  0.4069  0.4373 -0.0097\n",
      "  0.443   0.1645 -0.3274 -0.4041  0.2918 -0.3685  0.4932  0.1089 -0.953\n",
      " -0.1733 -0.7682 -0.1954 -0.1287  0.228   0.6271  0.1985  0.361  -0.1081\n",
      " -0.0824  0.4307  0.1899  0.0956  0.3014 -0.2754  0.4603  0.1531 -0.3867\n",
      "  0.3361 -0.3121 -0.1452  0.0548 -0.2079  0.1326  0.5235 -0.6903 -0.2719\n",
      " -0.6714  0.1533  0.3484  0.4001  0.3679  0.9936 -0.044   0.2208  0.0143\n",
      " -0.3165 -0.2096  0.2287  0.0054 -0.012  -0.671   0.7126 -0.4271  0.3308\n",
      "  0.1863 -0.0419 -0.8397 -0.3337 -0.1052  0.189   0.6441  0.2664  0.2566\n",
      " -0.297   0.4886  0.2538 -0.1068 -0.1875  0.2789  0.4644  0.4777 -0.1135\n",
      "  0.3931 -0.5544 -0.0817]\n"
     ]
    }
   ],
   "source": [
    "print(crawl_embeddings['Schwab'])\n",
    "print(crawl_embeddings['rmd'])\n",
    "print(crawl_embeddings['vanguard'])\n",
    "print(crawl_embeddings['blackrock'])\n",
    "print(crawl_embeddings['ameritrade'])\n",
    "print(crawl_embeddings['SCHW'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " try a \"lower/upper case version of a\" word if an embedding is not found, which sometimes gives us an embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'crawl_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a2bddea711e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrawl_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SchwaB'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'crawl_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "print(crawl_embeddings['SchwaB'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(glove_embeddings['SchwaB'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_matrix(word_index, embedding_index):\n",
    "    unknown_words = []\n",
    "    lower_words = []\n",
    "    title_words = []\n",
    "    known_words = []\n",
    "    \n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        if i <= max_features:\n",
    "            try:\n",
    "                val = embedding_index[word]\n",
    "                known_words.append(word)\n",
    "            except KeyError:\n",
    "                try:\n",
    "                    val = embedding_index[word.lower()]\n",
    "                    lower_words.append(word)\n",
    "                except KeyError:\n",
    "                    try:\n",
    "                        val = embedding_index[word.title()]\n",
    "                        title_words.append(word)\n",
    "                    except KeyError:\n",
    "                        unknown_words.append(word)\n",
    "    return lower_words, title_words, unknown_words, known_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 400000\n",
    "tokenizer = text.Tokenizer(num_words = max_features, filters='',lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n lower words (crawl):  843\n",
      "n title words (crawl):  667\n",
      "n unknown words (crawl):  14318\n",
      "n known words (crawl):  84438\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer.fit_on_texts(list(train['comment_text']))\n",
    "\n",
    "lower_words, title_words, unknown_words, known_words = build_matrix(tokenizer.word_index, crawl_embeddings)\n",
    "print('n lower words (crawl): ', len(lower_words))\n",
    "print('n title words (crawl): ', len(title_words))\n",
    "print('n unknown words (crawl): ', len(unknown_words))\n",
    "print('n known words (crawl): ', len(known_words))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n lower words (glove):  738\n",
      "n title words (glove):  572\n",
      "n unknown words (glove):  14598\n",
      "n known words (glove):  84358\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lower_words, title_words, unknown_words, known_words = build_matrix(tokenizer.word_index, glove_embeddings)\n",
    "print('n lower words (glove): ', len(lower_words))\n",
    "print('n title words (glove): ', len(title_words))\n",
    "print('n unknown words (glove): ', len(unknown_words))\n",
    "print('n known words (glove): ', len(known_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols_to_isolate = '.,?!-;*\"…:—()%#$&_/@＼・ω+=”“[]^–>\\\\°<~•≠™ˈʊɒ∞§{}·τα❤☺ɡ|¢→̶`❥━┣┫┗Ｏ►★©―ɪ✔®\\x96\\x92●£♥➤´¹☕≈÷♡◐║▬′ɔː€۩۞†μ✒➥═☆ˌ◄½ʻπδηλσερνʃ✬ＳＵＰＥＲＩＴ☻±♍µº¾✓◾؟．⬅℅»Вав❣⋅¿¬♫ＣＭβ█▓▒░⇒⭐›¡₂₃❧▰▔◞▀▂▃▄▅▆▇↙γ̄″☹➡«φ⅓„✋：¥̲̅́∙‛◇✏▷❓❗¶˚˙）сиʿ✨。ɑ\\x80◕！％¯−ﬂﬁ₁²ʌ¼⁴⁄₄⌠♭✘╪▶☭✭♪☔☠♂☃☎✈✌✰❆☙○‣⚓年∎ℒ▪▙☏⅛ｃａｓǀ℮¸ｗ‚∼‖ℳ❄←☼⋆ʒ⊂、⅔¨͡๏⚾⚽Φ×θ￦？（℃⏩☮⚠月✊❌⭕▸■⇌☐☑⚡☄ǫ╭∩╮，例＞ʕɐ̣Δ₀✞┈╱╲▏▕┃╰▊▋╯┳┊≥☒↑☝ɹ✅☛♩☞ＡＪＢ◔◡↓♀⬆̱ℏ\\x91⠀ˤ╚↺⇤∏✾◦♬³の｜／∵∴√Ω¤☜▲↳▫‿⬇✧ｏｖｍ－２０８＇‰≤∕ˆ⚜☁'\n",
    "symbols_to_delete = '\\n🍕\\r🐵😑\\xa0\\ue014\\t\\uf818\\uf04a\\xad😢🐶️\\uf0e0😜😎👊\\u200b\\u200e😁عدويهصقأناخلىبمغر😍💖💵Е👎😀😂\\u202a\\u202c🔥😄🏻💥ᴍʏʀᴇɴᴅᴏᴀᴋʜᴜʟᴛᴄᴘʙғᴊᴡɢ😋👏שלוםבי😱‼\\x81エンジ故障\\u2009🚌ᴵ͞🌟😊😳😧🙀😐😕\\u200f👍😮😃😘אעכח💩💯⛽🚄🏼ஜ😖ᴠ🚲‐😟😈💪🙏🎯🌹😇💔😡\\x7f👌ἐὶήιὲκἀίῃἴξ🙄Ｈ😠\\ufeff\\u2028😉😤⛺🙂\\u3000تحكسة👮💙فزط😏🍾🎉😞\\u2008🏾😅😭👻😥😔😓🏽🎆🍻🍽🎶🌺🤔😪\\x08‑🐰🐇🐱🙆😨🙃💕𝘊𝘦𝘳𝘢𝘵𝘰𝘤𝘺𝘴𝘪𝘧𝘮𝘣💗💚地獄谷улкнПоАН🐾🐕😆ה🔗🚽歌舞伎🙈😴🏿🤗🇺🇸мυтѕ⤵🏆🎃😩\\u200a🌠🐟💫💰💎эпрд\\x95🖐🙅⛲🍰🤐👆🙌\\u2002💛🙁👀🙊🙉\\u2004ˢᵒʳʸᴼᴷᴺʷᵗʰᵉᵘ\\x13🚬🤓\\ue602😵άοόςέὸתמדףנרךצט😒͝🆕👅👥👄🔄🔤👉👤👶👲🔛🎓\\uf0b7\\uf04c\\x9f\\x10成都😣⏺😌🤑🌏😯ех😲Ἰᾶὁ💞🚓🔔📚🏀👐\\u202d💤🍇\\ue613小土豆🏡❔⁉\\u202f👠》कर्मा🇹🇼🌸蔡英文🌞🎲レクサス😛外国人关系Сб💋💀🎄💜🤢َِьыгя不是\\x9c\\x9d🗑\\u2005💃📣👿༼つ༽😰ḷЗз▱ц￼🤣卖温哥华议会下降你失去所有的钱加拿大坏税骗子🐝ツ🎅\\x85🍺آإشء🎵🌎͟ἔ油别克🤡🤥😬🤧й\\u2003🚀🤴ʲшчИОРФДЯМюж😝🖑ὐύύ特殊作戦群щ💨圆明园קℐ🏈😺🌍⏏ệ🍔🐮🍁🍆🍑🌮🌯🤦\\u200d𝓒𝓲𝓿𝓵안영하세요ЖљКћ🍀😫🤤ῦ我出生在了可以说普通话汉语好极🎼🕺🍸🥂🗽🎇🎊🆘🤠👩🖒🚪天一家⚲\\u2006⚭⚆⬭⬯⏖新✀╌🇫🇷🇩🇪🇮🇬🇧😷🇨🇦ХШ🌐\\x1f杀鸡给猴看ʁ𝗪𝗵𝗲𝗻𝘆𝗼𝘂𝗿𝗮𝗹𝗶𝘇𝗯𝘁𝗰𝘀𝘅𝗽𝘄𝗱📺ϖ\\u2000үսᴦᎥһͺ\\u2007հ\\u2001ɩｙｅ൦ｌƽｈ𝐓𝐡𝐞𝐫𝐮𝐝𝐚𝐃𝐜𝐩𝐭𝐢𝐨𝐧Ƅᴨןᑯ໐ΤᏧ௦Іᴑ܁𝐬𝐰𝐲𝐛𝐦𝐯𝐑𝐙𝐣𝐇𝐂𝐘𝟎ԜТᗞ౦〔Ꭻ𝐳𝐔𝐱𝟔𝟓𝐅🐋ﬃ💘💓ё𝘥𝘯𝘶💐🌋🌄🌅𝙬𝙖𝙨𝙤𝙣𝙡𝙮𝙘𝙠𝙚𝙙𝙜𝙧𝙥𝙩𝙪𝙗𝙞𝙝𝙛👺🐷ℋ𝐀𝐥𝐪🚶𝙢Ἱ🤘ͦ💸ج패티Ｗ𝙇ᵻ👂👃ɜ🎫\\uf0a7БУі🚢🚂ગુજરાતીῆ🏃𝓬𝓻𝓴𝓮𝓽𝓼☘﴾̯﴿₽\\ue807𝑻𝒆𝒍𝒕𝒉𝒓𝒖𝒂𝒏𝒅𝒔𝒎𝒗𝒊👽😙\\u200cЛ‒🎾👹⎌🏒⛸公寓养宠物吗🏄🐀🚑🤷操美𝒑𝒚𝒐𝑴🤙🐒欢迎来到阿拉斯ספ𝙫🐈𝒌𝙊𝙭𝙆𝙋𝙍𝘼𝙅ﷻ🦄巨收赢得白鬼愤怒要买额ẽ🚗🐳𝟏𝐟𝟖𝟑𝟕𝒄𝟗𝐠𝙄𝙃👇锟斤拷𝗢𝟳𝟱𝟬⦁マルハニチロ株式社⛷한국어ㄸㅓ니͜ʖ𝘿𝙔₵𝒩ℯ𝒾𝓁𝒶𝓉𝓇𝓊𝓃𝓈𝓅ℴ𝒻𝒽𝓀𝓌𝒸𝓎𝙏ζ𝙟𝘃𝗺𝟮𝟭𝟯𝟲👋🦊多伦🐽🎻🎹⛓🏹🍷🦆为和中友谊祝贺与其想象对法如直接问用自己猜本传教士没积唯认识基督徒曾经让相信耶稣复活死怪他但当们聊些政治题时候战胜因圣把全堂结婚孩恐惧且栗谓这样还♾🎸🤕🤒⛑🎁批判检讨🏝🦁🙋😶쥐스탱트뤼도석유가격인상이경제황을렵게만들지않록잘관리해야합다캐나에서대마초와화약금의품런성분갈때는반드시허된사용🔫👁凸ὰ💲🗯𝙈Ἄ𝒇𝒈𝒘𝒃𝑬𝑶𝕾𝖙𝖗𝖆𝖎𝖌𝖍𝖕𝖊𝖔𝖑𝖉𝖓𝖐𝖜𝖞𝖚𝖇𝕿𝖘𝖄𝖛𝖒𝖋𝖂𝕴𝖟𝖈𝕸👑🚿💡知彼百\\uf005𝙀𝒛𝑲𝑳𝑾𝒋𝟒😦𝙒𝘾𝘽🏐𝘩𝘨ὼṑ𝑱𝑹𝑫𝑵𝑪🇰🇵👾ᓇᒧᔭᐃᐧᐦᑳᐨᓃᓂᑲᐸᑭᑎᓀᐣ🐄🎈🔨🐎🤞🐸💟🎰🌝🛳点击查版🍭𝑥𝑦𝑧ＮＧ👣\\uf020っ🏉ф💭🎥Ξ🐴👨🤳🦍\\x0b🍩𝑯𝒒😗𝟐🏂👳🍗🕉🐲چی𝑮𝗕𝗴🍒ꜥⲣⲏ🐑⏰鉄リ事件ї💊「」\\uf203\\uf09a\\uf222\\ue608\\uf202\\uf099\\uf469\\ue607\\uf410\\ue600燻製シ虚偽屁理屈Г𝑩𝑰𝒀𝑺🌤𝗳𝗜𝗙𝗦𝗧🍊ὺἈἡχῖΛ⤏🇳𝒙ψՁմեռայինրւդձ冬至ὀ𝒁🔹🤚🍎𝑷🐂💅𝘬𝘱𝘸𝘷𝘐𝘭𝘓𝘖𝘹𝘲𝘫کΒώ💢ΜΟΝΑΕ🇱♲𝝈↴💒⊘Ȼ🚴🖕🖤🥘📍👈➕🚫🎨🌑🐻𝐎𝐍𝐊𝑭🤖🎎😼🕷ｇｒｎｔｉｄｕｆｂｋ𝟰🇴🇭🇻🇲𝗞𝗭𝗘𝗤👼📉🍟🍦🌈🔭《🐊🐍\\uf10aლڡ🐦\\U0001f92f\\U0001f92a🐡💳ἱ🙇𝗸𝗟𝗠𝗷🥜さようなら🔼'\n",
    "\n",
    "\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "\n",
    "isolate_dict = {ord(c):f' {c} ' for c in symbols_to_isolate}\n",
    "remove_dict = {ord(c):f'' for c in symbols_to_delete}\n",
    "\n",
    "\n",
    "def handle_punctuation(x):\n",
    "    x = x.translate(remove_dict)\n",
    "    x = x.translate(isolate_dict)\n",
    "    return x\n",
    "\n",
    "def handle_contractions(x):\n",
    "    x = tokenizer.tokenize(x)\n",
    "    return x\n",
    "\n",
    "def fix_quote(x):\n",
    "    x = [x_[1:] if x_.startswith(\"'\") else x_ for x_ in x]\n",
    "    x = ' '.join(x)\n",
    "    return x\n",
    "\n",
    "def preprocess(x):\n",
    "    x = handle_punctuation(x)\n",
    "    x = handle_contractions(x)\n",
    "    x = fix_quote(x)\n",
    "    return x\n",
    "\n",
    "train['comment_text'] = train['comment_text'].apply(lambda x:preprocess(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(num_words = max_features, filters='',lower=False)\n",
    "tokenizer.fit_on_texts(list(train['comment_text']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n lower words (crawl):  842\n",
      "n title words (crawl):  667\n",
      "n unknown words (crawl):  14287\n",
      "n known words (crawl):  84412\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer.fit_on_texts(list(train['comment_text']))\n",
    "\n",
    "lower_words, title_words, unknown_words, known_words = build_matrix(tokenizer.word_index, crawl_embeddings)\n",
    "print('n lower words (crawl): ', len(lower_words))\n",
    "print('n title words (crawl): ', len(title_words))\n",
    "print('n unknown words (crawl): ', len(unknown_words))\n",
    "print('n known words (crawl): ', len(known_words))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n lower words (glove):  738\n",
      "n title words (glove):  572\n",
      "n unknown words (glove):  14540\n",
      "n known words (glove):  84358\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lower_words, title_words, unknown_words, known_words = build_matrix(tokenizer.word_index, glove_embeddings)\n",
    "print('n lower words (glove): ', len(lower_words))\n",
    "print('n title words (glove): ', len(title_words))\n",
    "print('n unknown words (glove): ', len(unknown_words))\n",
    "print('n known words (glove): ', len(known_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'122MM',\n",
       " '17Million',\n",
       " '20End',\n",
       " '20FINAL',\n",
       " '20Letter',\n",
       " '250Million',\n",
       " '25YRS',\n",
       " '2Cm',\n",
       " '2Cv',\n",
       " '3MILLION',\n",
       " '3Wd',\n",
       " '40Years',\n",
       " '7Million',\n",
       " 'AAAhhh',\n",
       " 'AAaaa',\n",
       " 'ACCESIBLE',\n",
       " 'ACIDIFICATION',\n",
       " 'ADVOCATED',\n",
       " 'AFterall',\n",
       " 'ALASKAS',\n",
       " \"ALI'I\",\n",
       " 'ALLUDE',\n",
       " 'ALLof',\n",
       " 'ALaskans',\n",
       " 'AMORIS',\n",
       " 'ANAE',\n",
       " 'APOLLOS',\n",
       " 'APPEASER',\n",
       " 'APr',\n",
       " 'ARRET',\n",
       " 'ASo',\n",
       " 'ATTENDENTS',\n",
       " 'Accelerants',\n",
       " 'Actially',\n",
       " 'Adance',\n",
       " 'Afirmative',\n",
       " 'Agirl',\n",
       " 'AirBnb',\n",
       " 'AlASKANS',\n",
       " 'AlCan',\n",
       " 'Aliado',\n",
       " 'AllenE',\n",
       " 'Allyou',\n",
       " 'AmIright',\n",
       " 'AmericaNo',\n",
       " 'Ampla',\n",
       " 'And2',\n",
       " 'AntiFA',\n",
       " 'AntiFa',\n",
       " 'Antivaxxers',\n",
       " 'Anyay',\n",
       " 'ArmyMan',\n",
       " 'AtheO',\n",
       " 'Atrack',\n",
       " 'Atually',\n",
       " 'AustOn',\n",
       " 'Ayaa',\n",
       " 'BAKKA',\n",
       " 'BANISHMENT',\n",
       " 'BArry',\n",
       " 'BEEJEEZUS',\n",
       " 'BILLON',\n",
       " 'BIden',\n",
       " 'BLASE',\n",
       " 'BLINDINGLY',\n",
       " 'BOff',\n",
       " 'BRAINIER',\n",
       " 'BRIBING',\n",
       " 'BRv',\n",
       " 'BUTHe',\n",
       " 'BaHaHaHa',\n",
       " 'Backslapping',\n",
       " 'BanKRupt',\n",
       " 'Barbarianism',\n",
       " 'BeIn',\n",
       " 'Befuddles',\n",
       " 'Billlion',\n",
       " 'BizJournal',\n",
       " 'Blablablah',\n",
       " 'Blahhhhh',\n",
       " 'BoRs',\n",
       " 'BradFord',\n",
       " 'Buccaneering',\n",
       " 'Buggah',\n",
       " 'Bullpucky',\n",
       " 'CALTrans',\n",
       " 'CANNIBIS',\n",
       " 'CATALONIAN',\n",
       " 'CATHEDRA',\n",
       " 'CAVERS',\n",
       " 'CAtholic',\n",
       " 'CEDED',\n",
       " 'CH2m',\n",
       " 'CIAis',\n",
       " 'CIte',\n",
       " 'COLDer',\n",
       " 'COLLUDED',\n",
       " 'COMMNTS',\n",
       " 'CONFORMIST',\n",
       " 'CONSTRUCTON',\n",
       " 'CONServatives',\n",
       " 'CONTRACTUALLY',\n",
       " 'CONartist',\n",
       " 'CONservatism',\n",
       " 'CONveniently',\n",
       " 'COUNCILMEMBERS',\n",
       " 'COUNT3',\n",
       " 'COx',\n",
       " 'CRISSAKES',\n",
       " 'CULTURALlY',\n",
       " 'Cablecos',\n",
       " 'Capial',\n",
       " 'Catastrophists',\n",
       " 'CeTA',\n",
       " 'ChaPMAN',\n",
       " 'ChairMan',\n",
       " 'CheckThe',\n",
       " 'Cheee',\n",
       " 'Cherrypicking',\n",
       " 'ChickenLittle',\n",
       " 'ChickenS',\n",
       " 'ClarK',\n",
       " 'Clealy',\n",
       " 'Comapred',\n",
       " 'Comradery',\n",
       " 'Concordats',\n",
       " 'Conservationism',\n",
       " 'Constructionists',\n",
       " 'Contrarianism',\n",
       " 'Corpoate',\n",
       " 'Coulds',\n",
       " 'CounciL',\n",
       " 'Counciler',\n",
       " 'Crappin',\n",
       " 'Cretinous',\n",
       " 'Crudity',\n",
       " 'Culpably',\n",
       " 'Curiousgeorge',\n",
       " \"D'apr\",\n",
       " 'DADAH',\n",
       " 'DBag',\n",
       " 'DEADY',\n",
       " 'DECIDEDLY',\n",
       " 'DECLAWING',\n",
       " 'DEFENSIVELY',\n",
       " 'DEFLECTING',\n",
       " 'DEMOCRATES',\n",
       " 'DEMOCRAZY',\n",
       " 'DETESTABLE',\n",
       " 'DEmocrats',\n",
       " 'DHd',\n",
       " 'DIAMETRICALLY',\n",
       " 'DIEMS',\n",
       " 'DISHONORABLE',\n",
       " 'DISINGENUOUS',\n",
       " 'DIsguisting',\n",
       " 'DONCHAKNOW',\n",
       " 'DOOOOOOO',\n",
       " 'DOTARD',\n",
       " 'DOuglas',\n",
       " 'DRAGGERS',\n",
       " 'DRIVERLESS',\n",
       " 'DRUGGIES',\n",
       " 'DUMPster',\n",
       " 'DUPed',\n",
       " 'DanceAbility',\n",
       " 'Darij',\n",
       " 'DebA',\n",
       " 'Defecits',\n",
       " 'Definitionally',\n",
       " 'Defition',\n",
       " 'Deforesting',\n",
       " 'Delusionals',\n",
       " 'DemoRATS',\n",
       " 'DemoRats',\n",
       " 'Denail',\n",
       " 'Depositum',\n",
       " 'Desalinisation',\n",
       " 'Descirbe',\n",
       " 'Devasted',\n",
       " 'Diin',\n",
       " 'DilBit',\n",
       " 'Dilbit',\n",
       " 'Dilettantish',\n",
       " 'Dimbulb',\n",
       " 'Dislexia',\n",
       " 'Dismissiveness',\n",
       " 'DixieCrats',\n",
       " 'Djt',\n",
       " 'Dministration',\n",
       " 'Doea',\n",
       " 'Doltish',\n",
       " 'Doso',\n",
       " 'Dotard',\n",
       " 'Dumbcrats',\n",
       " 'EDGARs',\n",
       " 'EEEEH',\n",
       " 'EFFUSIVE',\n",
       " 'ENTERY',\n",
       " 'EXACERBATE',\n",
       " 'EYes',\n",
       " 'Elor',\n",
       " 'Embezzlers',\n",
       " 'Emergecy',\n",
       " 'EminEm',\n",
       " 'Emphasing',\n",
       " 'Encephalomyopathy',\n",
       " 'Endzones',\n",
       " 'Equipmet',\n",
       " 'Erious',\n",
       " 'Essendo',\n",
       " 'Everyoe',\n",
       " 'Exacty',\n",
       " 'Excede',\n",
       " 'Extralegal',\n",
       " 'Exults',\n",
       " 'FALSEHOODS',\n",
       " 'FALSLY',\n",
       " 'FEBRILE',\n",
       " 'FIGURATIVELY',\n",
       " 'FILIUS',\n",
       " 'FLOWIN',\n",
       " 'FOIBLES',\n",
       " 'FREELOADING',\n",
       " 'FUCD',\n",
       " 'FUNNEE',\n",
       " 'Falderal',\n",
       " 'Fecklessness',\n",
       " 'Ferget',\n",
       " 'FlexSteel',\n",
       " 'Fluorosilicic',\n",
       " 'Folllowing',\n",
       " 'Foolhardily',\n",
       " 'Forestalled',\n",
       " 'Fortnights',\n",
       " 'Franchaise',\n",
       " 'Frought',\n",
       " 'FundI',\n",
       " 'GASSING',\n",
       " 'GAUDIUM',\n",
       " 'GLAMORIZE',\n",
       " 'GLANCING',\n",
       " 'GLOATING',\n",
       " 'GOSSIPING',\n",
       " 'GOVTS',\n",
       " 'GUNs',\n",
       " 'Gasify',\n",
       " 'Gettit',\n",
       " 'Glacials',\n",
       " 'Globalnews',\n",
       " 'Goatie',\n",
       " 'Grandgirls',\n",
       " 'Guesss',\n",
       " 'GunSight',\n",
       " 'HAAAAAAAAAAA',\n",
       " 'HAHAhahahahahahahahaha',\n",
       " 'HAMMS',\n",
       " \"HE'E\",\n",
       " 'HEIGHTEN',\n",
       " 'HEMIC',\n",
       " 'HERESAY',\n",
       " 'HINGTON',\n",
       " 'HInt',\n",
       " 'HOARDS',\n",
       " 'HOLLOWING',\n",
       " 'HOOEY',\n",
       " 'HOOLY',\n",
       " 'HOPERS',\n",
       " 'HORRENDOUSLY',\n",
       " 'HUUUUUUUGE',\n",
       " 'HaHahA',\n",
       " 'HaI',\n",
       " 'Haaaaaaaaa',\n",
       " 'Haaaaaaaaaaaaa',\n",
       " 'Hahahaahaha',\n",
       " 'Hahahahahahahahah',\n",
       " 'Headscissor',\n",
       " 'Hewers',\n",
       " 'HiLIARy',\n",
       " 'Hipocrits',\n",
       " 'Honset',\n",
       " 'Hyprocrites',\n",
       " 'IFthere',\n",
       " 'IMmigrant',\n",
       " 'INSHALLA',\n",
       " 'INTERPRETS',\n",
       " 'INTERVENOR',\n",
       " 'INouye',\n",
       " 'INsurance',\n",
       " 'INvoluntary',\n",
       " 'IRresponsible',\n",
       " 'IUt',\n",
       " 'Ijk',\n",
       " 'Immagrants',\n",
       " 'Incedible',\n",
       " 'Incisively',\n",
       " 'Incomers',\n",
       " 'Indegenous',\n",
       " 'Indigeneous',\n",
       " 'Indigenously',\n",
       " 'Indigineous',\n",
       " 'Ineradicable',\n",
       " 'Ingore',\n",
       " 'Insufficent',\n",
       " 'Intelegence',\n",
       " 'Intollerant',\n",
       " 'JackS',\n",
       " 'JayVee',\n",
       " 'JeffreyI',\n",
       " 'JustIce',\n",
       " 'KAAA',\n",
       " 'KANAKA',\n",
       " 'KESEY',\n",
       " 'KUDOs',\n",
       " 'Keisters',\n",
       " 'Kilolitres',\n",
       " 'Kolego',\n",
       " 'KpI',\n",
       " \"L'actrice\",\n",
       " 'LAIM',\n",
       " 'LAMESTREAM',\n",
       " 'LATAE',\n",
       " 'LAUGHINGLY',\n",
       " 'LEFTIE',\n",
       " 'LENTS',\n",
       " 'LIBTURD',\n",
       " 'LIOs',\n",
       " 'LIbErals',\n",
       " 'LIberalism',\n",
       " 'LIbya',\n",
       " 'LOLIn',\n",
       " 'LOLIts',\n",
       " 'LOLWhat',\n",
       " 'LONO',\n",
       " 'LRon',\n",
       " 'LUDDITES',\n",
       " 'LUNGWORM',\n",
       " 'LYou',\n",
       " 'Lakum',\n",
       " 'LeTang',\n",
       " 'Legistlature',\n",
       " 'Libbers',\n",
       " 'Limewater',\n",
       " 'Litteram',\n",
       " 'Litterers',\n",
       " 'Littleness',\n",
       " 'Liviing',\n",
       " 'MAINTANENCE',\n",
       " 'MCCarthy',\n",
       " 'MCConnell',\n",
       " 'MCoy',\n",
       " 'MERICANS',\n",
       " 'METAPHORES',\n",
       " 'MISCARRAIGE',\n",
       " 'MISDEMENOR',\n",
       " 'MIlLLION',\n",
       " 'MIning',\n",
       " 'MIssile',\n",
       " 'MONITARY',\n",
       " 'MONUMENTALLY',\n",
       " 'MOOCHING',\n",
       " 'MUZZIE',\n",
       " 'MXIT',\n",
       " 'MaCarthy',\n",
       " 'Malasada',\n",
       " 'MartE',\n",
       " 'MasSu',\n",
       " 'MatCom',\n",
       " 'Matterwhat',\n",
       " 'Medevaced',\n",
       " 'MedicAid',\n",
       " 'Menials',\n",
       " 'Mimicus',\n",
       " 'Misappropriates',\n",
       " 'Mismanagment',\n",
       " 'Mispronunciation',\n",
       " 'Mod3',\n",
       " 'Mountainscape',\n",
       " 'MuskOx',\n",
       " 'Muslimes',\n",
       " 'MyOpinion',\n",
       " 'NAnos',\n",
       " 'NEWSMEDIA',\n",
       " 'NINCOMPOOPERY',\n",
       " 'NONES',\n",
       " 'NORMIE',\n",
       " 'NOUN1',\n",
       " 'NSNBC',\n",
       " 'NUTCASES',\n",
       " 'NYAAA',\n",
       " 'Narritive',\n",
       " 'NationaL',\n",
       " 'NcR',\n",
       " 'NeoLiberals',\n",
       " 'NeoLibs',\n",
       " 'NeoNazi',\n",
       " 'Newspeople',\n",
       " 'NoThey',\n",
       " 'Nohting',\n",
       " 'Nothingburger',\n",
       " 'Nowonder',\n",
       " 'NtA',\n",
       " 'Numpties',\n",
       " 'Nutbar',\n",
       " 'OBAMAcare',\n",
       " 'OBLIGATE',\n",
       " 'OBSERVABLY',\n",
       " 'OBummer',\n",
       " 'OCONNER',\n",
       " 'OHno',\n",
       " 'OKOLE',\n",
       " 'OLive',\n",
       " 'ONtario',\n",
       " 'OOOOOHH',\n",
       " 'OOOoooo',\n",
       " 'ORDAIN',\n",
       " 'OTEHR',\n",
       " 'OVERTHREW',\n",
       " 'OVERjoyed',\n",
       " 'Ofay',\n",
       " 'Oilies',\n",
       " 'OkI',\n",
       " 'Onced',\n",
       " 'Opaqueness',\n",
       " 'Opprobrious',\n",
       " 'OrYour',\n",
       " 'Orangeness',\n",
       " 'Organigram',\n",
       " 'Orhttp',\n",
       " 'OverThinking',\n",
       " 'Overanalyzing',\n",
       " 'Overextending',\n",
       " 'PAIDEIA',\n",
       " 'PATENTLY',\n",
       " 'PAYCHEQUE',\n",
       " 'PAYOR',\n",
       " 'PArdon',\n",
       " 'PERSECUTE',\n",
       " 'PERSONIFICATION',\n",
       " 'PHUK',\n",
       " 'PHat',\n",
       " 'PITTANCE',\n",
       " 'PIg',\n",
       " 'POLiTICALLY',\n",
       " 'POPULATON',\n",
       " 'POPULISM',\n",
       " 'POSTAGEM',\n",
       " 'POTHEADS',\n",
       " 'PRETENSES',\n",
       " 'PROBABLILITY',\n",
       " 'PROPORTIONATELY',\n",
       " 'PROVABLY',\n",
       " 'PROVINCIALISM',\n",
       " 'PRYING',\n",
       " 'PanHandler',\n",
       " 'Partenership',\n",
       " 'PayMe',\n",
       " 'Peiod',\n",
       " 'Pences',\n",
       " 'Perichoresis',\n",
       " 'Pespective',\n",
       " 'Polyacrylamides',\n",
       " 'Pooof',\n",
       " 'Pooooor',\n",
       " 'PostLiberal',\n",
       " 'Poting',\n",
       " 'Prattling',\n",
       " 'Prescripiton',\n",
       " 'Pretention',\n",
       " 'Proselytization',\n",
       " 'Puton',\n",
       " 'Qrtr',\n",
       " 'RAcist',\n",
       " 'RECALCULATING',\n",
       " 'RECKLESSLY',\n",
       " 'REFRESHINGLY',\n",
       " 'REHABILITATED',\n",
       " 'RESISTER',\n",
       " 'REpubs',\n",
       " 'RIDICULOUSNESS',\n",
       " 'RTHur',\n",
       " 'Rapaciousness',\n",
       " 'RedSquare',\n",
       " 'Reearch',\n",
       " 'Reevaluations',\n",
       " 'Regardng',\n",
       " 'Reinstitute',\n",
       " 'Rejectionists',\n",
       " 'Relabeled',\n",
       " 'RepubliCrat',\n",
       " 'Responsility',\n",
       " 'Revenuers',\n",
       " 'RiCO',\n",
       " 'Rithmatic',\n",
       " 'RnA',\n",
       " 'RoboCalls',\n",
       " 'Rousting',\n",
       " 'RuTH',\n",
       " 'RustBelt',\n",
       " 'SANELY',\n",
       " 'SCOWLING',\n",
       " 'SChumer',\n",
       " 'SELFIE',\n",
       " 'SENSational',\n",
       " 'SENSible',\n",
       " 'SENTENTIAE',\n",
       " 'SHAMEFULLY',\n",
       " 'SHAMING',\n",
       " 'SHOPOHOLIC',\n",
       " 'SHTT',\n",
       " 'SICKEN',\n",
       " 'SINce',\n",
       " 'SISs',\n",
       " 'SLANDERED',\n",
       " 'SMETHING',\n",
       " 'SOCIALITES',\n",
       " 'SOSs',\n",
       " 'SPIECES',\n",
       " 'SPIKING',\n",
       " 'ST00PID',\n",
       " 'STATISTS',\n",
       " 'STatistics',\n",
       " 'STaying',\n",
       " 'STig',\n",
       " 'SUBSIDIZING',\n",
       " 'SUBVERTED',\n",
       " 'SUPPOrt',\n",
       " 'SYNGAS',\n",
       " 'Sad2',\n",
       " 'Salaciousness',\n",
       " 'Salutatorians',\n",
       " 'Scfi',\n",
       " 'Schocking',\n",
       " 'Sciencey',\n",
       " 'Scumbucket',\n",
       " 'Seadoos',\n",
       " 'Seiously',\n",
       " 'Semisub',\n",
       " 'Shamful',\n",
       " 'ShaunA',\n",
       " 'Shellfire',\n",
       " 'Shiftiness',\n",
       " 'Shov',\n",
       " 'Signoffs',\n",
       " 'Silicofluorides',\n",
       " 'SitCom',\n",
       " 'Sloooooowly',\n",
       " 'Smeels',\n",
       " 'Snickered',\n",
       " 'Sobeit',\n",
       " 'Socons',\n",
       " 'Soweth',\n",
       " 'Spitefully',\n",
       " 'Spoolable',\n",
       " 'Springeth',\n",
       " 'Sroll',\n",
       " 'Subducting',\n",
       " 'Subract',\n",
       " 'SuperDelegates',\n",
       " 'SuperGrid',\n",
       " 'Superflous',\n",
       " 'Supremicist',\n",
       " 'TERRIS',\n",
       " 'THEROCK',\n",
       " 'THUMPED',\n",
       " 'TOWNERS',\n",
       " 'TRANSLINK',\n",
       " 'TRIBALISM',\n",
       " 'TRUMPER',\n",
       " 'TRUMPet',\n",
       " 'TRump',\n",
       " 'TUf',\n",
       " 'TWIGGED',\n",
       " 'Temperary',\n",
       " 'Terriorist',\n",
       " 'Terrrorism',\n",
       " 'Testifiers',\n",
       " 'Theydo',\n",
       " 'Theymay',\n",
       " 'Thuh',\n",
       " 'ToTL',\n",
       " 'Totlly',\n",
       " 'Trickledown',\n",
       " 'Triteness',\n",
       " 'Trivialize',\n",
       " 'Trole',\n",
       " 'Trumpery',\n",
       " 'Tussocks',\n",
       " 'UGY',\n",
       " 'UNACCOUNTABLE',\n",
       " 'UNAFFORDABLE',\n",
       " 'UNCIVIL',\n",
       " 'UNCLEANABLE',\n",
       " 'UNDERSTAFFING',\n",
       " 'UNEQUIVOCALLY',\n",
       " 'UNIONIZED',\n",
       " 'UNMASKING',\n",
       " 'UNMASKS',\n",
       " 'UNREF',\n",
       " 'UNRELENTING',\n",
       " 'UNTRUTHFUL',\n",
       " 'UNdemocratic',\n",
       " 'UNionism',\n",
       " 'UNworthiness',\n",
       " 'UScCB',\n",
       " 'Uhhhhhhhhh',\n",
       " 'Unchipped',\n",
       " 'Uneeded',\n",
       " 'Unhoused',\n",
       " 'Universalistic',\n",
       " 'Unknowledgeable',\n",
       " 'Unlesss',\n",
       " 'UoT',\n",
       " 'Uppp',\n",
       " 'VIRE',\n",
       " 'VOL01',\n",
       " 'Vacillates',\n",
       " 'Vva',\n",
       " 'WALLe',\n",
       " 'WATCHIT',\n",
       " 'WAVEMAKER',\n",
       " 'WHACKO',\n",
       " 'WHITEhouse',\n",
       " 'WHine',\n",
       " 'WORLDNET',\n",
       " 'WQe',\n",
       " 'WRANGLE',\n",
       " 'WaPO',\n",
       " 'WarMongers',\n",
       " 'Welching',\n",
       " 'WhT',\n",
       " 'Wherin',\n",
       " 'Whinner',\n",
       " 'WhiteFace',\n",
       " 'Wimping',\n",
       " 'Wootwoot',\n",
       " 'XBOT',\n",
       " 'YTHING',\n",
       " 'YUGE',\n",
       " 'YYour',\n",
       " 'YuCH',\n",
       " 'ZWH',\n",
       " 'ZZZZZzzzzzzz',\n",
       " 'abC',\n",
       " 'adI',\n",
       " 'aiD',\n",
       " 'andFree',\n",
       " 'andIf',\n",
       " 'andOnly',\n",
       " 'andPeople',\n",
       " 'aspI',\n",
       " 'baitA',\n",
       " 'busHey',\n",
       " 'businessNews',\n",
       " 'cANCer',\n",
       " 'cATHOLIC',\n",
       " 'cHURCH',\n",
       " 'cRUSE',\n",
       " 'captivatiNG',\n",
       " 'dOOd',\n",
       " 'dataFrom',\n",
       " 'deAd',\n",
       " 'deNile',\n",
       " 'deadHead',\n",
       " 'demoRat',\n",
       " 'democRats',\n",
       " 'detAil',\n",
       " 'doUS',\n",
       " 'downDown',\n",
       " 'eKo',\n",
       " 'eNCA',\n",
       " 'eighTEEN',\n",
       " 'eleAnor',\n",
       " 'endOf',\n",
       " 'ethNic',\n",
       " 'fAR',\n",
       " 'fInd',\n",
       " 'fRANCIS',\n",
       " 'fascinatiNG',\n",
       " 'fromIt',\n",
       " 'hILLary',\n",
       " 'hIllary',\n",
       " 'hiLIARy',\n",
       " 'horE',\n",
       " 'houseHold',\n",
       " 'intoThe',\n",
       " 'jCp',\n",
       " 'jrB',\n",
       " 'justBecause',\n",
       " 'kNOWN',\n",
       " 'mILLION',\n",
       " 'mIddle',\n",
       " 'mTPA',\n",
       " 'mailI',\n",
       " 'millionS',\n",
       " 'neoLiberals',\n",
       " 'neoNazi',\n",
       " 'neoNazis',\n",
       " 'nineTEEN',\n",
       " 'nothiNG',\n",
       " 'orginIzations',\n",
       " 'oxyMORON',\n",
       " 'pAid',\n",
       " 'pResidency',\n",
       " 'paTH',\n",
       " 'pageLet',\n",
       " 'partyIn',\n",
       " 'pdfPage',\n",
       " 'plIt',\n",
       " 'polIcy',\n",
       " 'reAd',\n",
       " 'readAnd',\n",
       " 'rowI',\n",
       " 'sJust',\n",
       " 'shoesAnd',\n",
       " 'sinA',\n",
       " 'speecheS',\n",
       " 'srcID',\n",
       " 'sylLABle',\n",
       " 'tAb',\n",
       " 'tOLERANCE',\n",
       " 'tRUMP',\n",
       " 'tRump',\n",
       " 'tRumps',\n",
       " 'theChild',\n",
       " 'themAs',\n",
       " 'themOn',\n",
       " 'thereIs',\n",
       " 'toCover',\n",
       " 'unCon',\n",
       " 'unGodliness',\n",
       " 'unPrepared',\n",
       " 'unSound',\n",
       " 'uponThe',\n",
       " 'usAs',\n",
       " 'waLL',\n",
       " 'whiteAnd',\n",
       " 'zERO'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(lower_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
